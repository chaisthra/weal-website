{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ANyMOiVi-e-A"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, GlobalMaxPooling2D,Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "uJY0pqjY-fA-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def redistribute_images(source_dir, train_dir, val_dir, test_size=0.2):\n",
    "    # Create directories if they do not exist\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.makedirs(train_dir)\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.makedirs(val_dir)\n",
    "\n",
    "    # List classes and distribute files\n",
    "    classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "    for cls in classes:\n",
    "        class_dir = os.path.join(source_dir, cls)\n",
    "        files = [os.path.join(class_dir, f) for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
    "\n",
    "        # Split files into training and validation\n",
    "        train_files, val_files = train_test_split(files, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Function to move files to their new directory\n",
    "        def move_files(files, dest):\n",
    "            os.makedirs(os.path.join(dest, cls), exist_ok=True)\n",
    "            for f in files:\n",
    "                shutil.move(f, os.path.join(dest, cls, os.path.basename(f)))\n",
    "\n",
    "        # Move files to respective directories\n",
    "        move_files(train_files, train_dir)\n",
    "        move_files(val_files, val_dir)\n",
    "\n",
    "# Paths setup\n",
    "source_directory = \"/Users/chaithra/tensorflow-test/Training copy\" # Path where all classes are currently stored\n",
    "training_directory = \"/Users/chaithra/tensorflow-test/training_directory\"\n",
    "validation_directory = \"/Users/chaithra/tensorflow-test/validation_directory\"\n",
    "\n",
    "# Redistribute images into training and validation sets\n",
    "redistribute_images(source_directory, training_directory, validation_directory, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "6GpMSOVs-fEB"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "image_size = 224\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "YYXZ_hcf-fG2",
    "outputId": "8627600c-cf79-491a-d684-0d1dabe422a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4568 images belonging to 4 classes.\n",
      "Found 1144 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def prepare_the_datasets(train_datasets, validation_datasets, batch_size, image_size):\n",
    "    # Augmentation for training data\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    # Normalization for validation data\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "    # Generate training data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_datasets,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Generate validation data\n",
    "    validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_datasets,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=\"categorical\",\n",
    "        shuffle=False  # No need to shuffle validation data\n",
    "    )\n",
    "\n",
    "    return train_generator, validation_generator\n",
    "\n",
    "# Usage of the function\n",
    "train_datasets = \"/Users/chaithra/tensorflow-test/training_directory\"\n",
    "validation_datasets = \"/Users/chaithra/tensorflow-test/validation_directory\"\n",
    "batch_size = 64\n",
    "image_size = 224\n",
    "\n",
    "train_data, validation_data = prepare_the_datasets(train_datasets, validation_datasets, batch_size, image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "LrDsQ_k--fMk"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (image_size, image_size, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(512, activation = \"relu\"),\n",
    "    Dense(4, activation = \"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "b4TXmTXt-fPT"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\"]\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "-xtB0csa-fSK"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath = \"model_checkpoint.h5\"\n",
    "callbacks_checkpoints = ModelCheckpoint(\n",
    "\n",
    "    filepath = model_checkpoint_filpath,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1eh-Po9-fU8",
    "outputId": "5e561ee0-6f78-4f75-b7a5-2f5ff1dcf8df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 26s 344ms/step - loss: 1.7504 - accuracy: 0.3667 - val_loss: 1.1016 - val_accuracy: 0.4904\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 25s 354ms/step - loss: 0.9382 - accuracy: 0.5987 - val_loss: 0.9949 - val_accuracy: 0.6136\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 26s 360ms/step - loss: 0.9387 - accuracy: 0.6095 - val_loss: 1.0795 - val_accuracy: 0.6722\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 0.8809 - accuracy: 0.6335 - val_loss: 1.0825 - val_accuracy: 0.6101\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.8337 - accuracy: 0.6631 - val_loss: 0.7265 - val_accuracy: 0.7002\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 0.9154 - accuracy: 0.6296 - val_loss: 1.6678 - val_accuracy: 0.5262\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 0.8867 - accuracy: 0.6581 - val_loss: 0.6988 - val_accuracy: 0.7448\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 27s 366ms/step - loss: 0.9879 - accuracy: 0.6462 - val_loss: 1.3788 - val_accuracy: 0.6390\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 25s 346ms/step - loss: 1.0457 - accuracy: 0.6329 - val_loss: 1.6476 - val_accuracy: 0.5542\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.9480 - accuracy: 0.6651 - val_loss: 1.0115 - val_accuracy: 0.6626\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 1.0717 - accuracy: 0.6727 - val_loss: 1.8532 - val_accuracy: 0.6372\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 1.3125 - accuracy: 0.6576 - val_loss: 3.6363 - val_accuracy: 0.4668\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 27s 368ms/step - loss: 1.7850 - accuracy: 0.6373 - val_loss: 5.0256 - val_accuracy: 0.6154\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 2.0678 - accuracy: 0.6300 - val_loss: 2.6199 - val_accuracy: 0.5935\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 25s 347ms/step - loss: 1.4559 - accuracy: 0.6657 - val_loss: 3.6361 - val_accuracy: 0.5970\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 2.1763 - accuracy: 0.6349 - val_loss: 3.0769 - val_accuracy: 0.6748\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 1.5873 - accuracy: 0.6754 - val_loss: 3.0848 - val_accuracy: 0.6346\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 2.4676 - accuracy: 0.6443 - val_loss: 2.5762 - val_accuracy: 0.7168\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 3.4740 - accuracy: 0.6283 - val_loss: 7.1515 - val_accuracy: 0.5149\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 4.0983 - accuracy: 0.6469 - val_loss: 4.8265 - val_accuracy: 0.6294\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 27s 368ms/step - loss: 5.6031 - accuracy: 0.6289 - val_loss: 10.2429 - val_accuracy: 0.5927\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 7.4131 - accuracy: 0.6189 - val_loss: 27.3527 - val_accuracy: 0.4816\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 26s 363ms/step - loss: 11.4247 - accuracy: 0.6182 - val_loss: 8.3499 - val_accuracy: 0.7483\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 14.6428 - accuracy: 0.6138 - val_loss: 17.8113 - val_accuracy: 0.7220\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 15.6872 - accuracy: 0.6359 - val_loss: 22.2142 - val_accuracy: 0.6906\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 27.5521 - accuracy: 0.6145 - val_loss: 54.5912 - val_accuracy: 0.5568\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 27s 367ms/step - loss: 44.5048 - accuracy: 0.6051 - val_loss: 109.8507 - val_accuracy: 0.6337\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 50.3676 - accuracy: 0.6138 - val_loss: 50.1152 - val_accuracy: 0.6486\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 51.5632 - accuracy: 0.6265 - val_loss: 145.9144 - val_accuracy: 0.4755\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 64.4458 - accuracy: 0.6235 - val_loss: 93.9341 - val_accuracy: 0.6434\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 26s 352ms/step - loss: 75.1883 - accuracy: 0.6346 - val_loss: 81.0608 - val_accuracy: 0.6713\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 82.5889 - accuracy: 0.6268 - val_loss: 82.7513 - val_accuracy: 0.6040\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 93.0820 - accuracy: 0.6281 - val_loss: 128.8548 - val_accuracy: 0.6337\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 247.8794 - accuracy: 0.5663 - val_loss: 304.1978 - val_accuracy: 0.5953\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 224.3511 - accuracy: 0.5915 - val_loss: 232.1328 - val_accuracy: 0.6495\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 230.3927 - accuracy: 0.6180 - val_loss: 359.2667 - val_accuracy: 0.5332\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 261.0212 - accuracy: 0.6141 - val_loss: 253.3460 - val_accuracy: 0.6608\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 329.6227 - accuracy: 0.6147 - val_loss: 437.4927 - val_accuracy: 0.5760\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 346.4713 - accuracy: 0.6329 - val_loss: 217.8266 - val_accuracy: 0.6879\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 374.6998 - accuracy: 0.6232 - val_loss: 312.9157 - val_accuracy: 0.7316\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 311.3566 - accuracy: 0.6550 - val_loss: 405.4764 - val_accuracy: 0.6267\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 605.1901 - accuracy: 0.5725 - val_loss: 645.1572 - val_accuracy: 0.6932\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 812.7565 - accuracy: 0.5889 - val_loss: 1158.6526 - val_accuracy: 0.5988\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 647.8934 - accuracy: 0.6300 - val_loss: 736.8937 - val_accuracy: 0.6748\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 829.1432 - accuracy: 0.6084 - val_loss: 604.3394 - val_accuracy: 0.7334\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 766.1857 - accuracy: 0.6261 - val_loss: 954.3326 - val_accuracy: 0.5760\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 880.3885 - accuracy: 0.6187 - val_loss: 1687.4061 - val_accuracy: 0.6740\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 1353.8884 - accuracy: 0.6018 - val_loss: 684.4029 - val_accuracy: 0.6879\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 1690.4059 - accuracy: 0.5944 - val_loss: 2810.3452 - val_accuracy: 0.5839\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 1539.6744 - accuracy: 0.6062 - val_loss: 4934.3052 - val_accuracy: 0.4703\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 2039.9443 - accuracy: 0.6070 - val_loss: 2042.1198 - val_accuracy: 0.6058\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 2128.9026 - accuracy: 0.6064 - val_loss: 4239.0562 - val_accuracy: 0.5288\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 1831.4740 - accuracy: 0.6213 - val_loss: 1234.1908 - val_accuracy: 0.7360\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 2605.6882 - accuracy: 0.5902 - val_loss: 4282.3374 - val_accuracy: 0.4843\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 2921.7488 - accuracy: 0.6020 - val_loss: 4918.5884 - val_accuracy: 0.5463\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 2588.5303 - accuracy: 0.6259 - val_loss: 5592.4268 - val_accuracy: 0.6154\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 3019.1143 - accuracy: 0.6219 - val_loss: 3303.1094 - val_accuracy: 0.6425\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 3522.8665 - accuracy: 0.6101 - val_loss: 6736.7095 - val_accuracy: 0.4659\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 26s 366ms/step - loss: 4791.7114 - accuracy: 0.5889 - val_loss: 5353.7910 - val_accuracy: 0.5516\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 5317.0923 - accuracy: 0.5898 - val_loss: 12992.3896 - val_accuracy: 0.4379\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 7269.2012 - accuracy: 0.5587 - val_loss: 8285.3613 - val_accuracy: 0.5149\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 8011.7559 - accuracy: 0.5773 - val_loss: 6471.1201 - val_accuracy: 0.6364\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 26s 360ms/step - loss: 6012.0664 - accuracy: 0.6125 - val_loss: 5111.5259 - val_accuracy: 0.6617\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 7178.4180 - accuracy: 0.5933 - val_loss: 5547.6797 - val_accuracy: 0.5979\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 25s 346ms/step - loss: 6222.2090 - accuracy: 0.6051 - val_loss: 9653.7373 - val_accuracy: 0.6643\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 5976.1196 - accuracy: 0.6224 - val_loss: 8987.3809 - val_accuracy: 0.5900\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 27s 369ms/step - loss: 9088.3623 - accuracy: 0.6051 - val_loss: 16140.3955 - val_accuracy: 0.5420\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 11821.7461 - accuracy: 0.5731 - val_loss: 15578.4336 - val_accuracy: 0.6757\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 11953.9209 - accuracy: 0.5922 - val_loss: 30409.7129 - val_accuracy: 0.5262\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 12596.5518 - accuracy: 0.5979 - val_loss: 13930.8887 - val_accuracy: 0.5105\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 16809.5605 - accuracy: 0.5617 - val_loss: 30174.5742 - val_accuracy: 0.4729\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 25s 347ms/step - loss: 13711.6787 - accuracy: 0.6077 - val_loss: 20221.6133 - val_accuracy: 0.4694\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 12484.2754 - accuracy: 0.6219 - val_loss: 19627.9121 - val_accuracy: 0.5385\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 25s 345ms/step - loss: 16442.3418 - accuracy: 0.5974 - val_loss: 23161.8066 - val_accuracy: 0.5105\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 17489.8223 - accuracy: 0.5893 - val_loss: 21963.7949 - val_accuracy: 0.6713\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 25306.2031 - accuracy: 0.5657 - val_loss: 47693.6836 - val_accuracy: 0.5323\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 25s 346ms/step - loss: 23141.2246 - accuracy: 0.5895 - val_loss: 16920.6094 - val_accuracy: 0.7089\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 20711.9668 - accuracy: 0.5989 - val_loss: 34429.8555 - val_accuracy: 0.4642\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 27150.3379 - accuracy: 0.5733 - val_loss: 26955.8730 - val_accuracy: 0.5822\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 27s 369ms/step - loss: 25324.6387 - accuracy: 0.6123 - val_loss: 22574.9121 - val_accuracy: 0.5498\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 29049.8477 - accuracy: 0.5797 - val_loss: 43150.3672 - val_accuracy: 0.5192\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 51142.1562 - accuracy: 0.5385 - val_loss: 80684.0078 - val_accuracy: 0.4449\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 61988.4609 - accuracy: 0.5455 - val_loss: 66681.9062 - val_accuracy: 0.6407\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 46980.4727 - accuracy: 0.5775 - val_loss: 72515.5000 - val_accuracy: 0.6128\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 25s 346ms/step - loss: 44039.5078 - accuracy: 0.5792 - val_loss: 69840.5781 - val_accuracy: 0.5341\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 42873.8594 - accuracy: 0.5751 - val_loss: 43953.5273 - val_accuracy: 0.6416\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 45513.0430 - accuracy: 0.5902 - val_loss: 40179.8438 - val_accuracy: 0.6547\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 43442.1055 - accuracy: 0.5904 - val_loss: 34922.2969 - val_accuracy: 0.6818\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 53672.2227 - accuracy: 0.5626 - val_loss: 34114.5156 - val_accuracy: 0.6521\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 45750.5312 - accuracy: 0.5871 - val_loss: 58459.2656 - val_accuracy: 0.5149\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 54933.8867 - accuracy: 0.5779 - val_loss: 72493.5859 - val_accuracy: 0.6031\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 27s 370ms/step - loss: 73135.0938 - accuracy: 0.5554 - val_loss: 67152.7969 - val_accuracy: 0.5577\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 59940.8633 - accuracy: 0.5757 - val_loss: 80478.2500 - val_accuracy: 0.5979\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 26s 360ms/step - loss: 69633.9609 - accuracy: 0.5817 - val_loss: 87839.6719 - val_accuracy: 0.6267\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 85152.4844 - accuracy: 0.5655 - val_loss: 190324.7500 - val_accuracy: 0.5944\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 97854.0156 - accuracy: 0.5657 - val_loss: 265842.0938 - val_accuracy: 0.4309\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 27s 371ms/step - loss: 149183.6719 - accuracy: 0.5210 - val_loss: 144806.8594 - val_accuracy: 0.5149\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 116296.5781 - accuracy: 0.5556 - val_loss: 84551.5547 - val_accuracy: 0.6713\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 25s 347ms/step - loss: 126252.4688 - accuracy: 0.5519 - val_loss: 230591.9531 - val_accuracy: 0.3601\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 150780.0938 - accuracy: 0.5471 - val_loss: 75218.6328 - val_accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sEX2GBCi-fXs",
    "outputId": "6517816c-d035-4030-d728-ec24c770c353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 65ms/step - loss: 75218.6328 - accuracy: 0.6538\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ict-0A8V-fbP",
    "outputId": "edbb26d1-c87c-41e4-d1bf-188bec5355bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_max_pooling2d_3 (Gl  (None, 2048)              0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 24638852 (93.99 MB)\n",
      "Trainable params: 1051140 (4.01 MB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "#ResNet50\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained ResNet101 model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.resnet.ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)\n",
    "\n",
    "# Freeze the layers of the pre-trained ResNet50 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your custom model on top of the pre-trained ResNet50\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalMaxPooling2D())  # Use GlobalMaxPooling2D instead of MaxPooling2D\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'binary' if it is two classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        lr = lr * 0.9  # Adjust the factor as needed\n",
    "    return lr\n",
    "\n",
    "# Use LearningRateScheduler during model training\n",
    "lr_schedule = LearningRateScheduler(lr_scheduler)\n",
    "optimizer = Adam(learning_rate=0.001)  # Adjust the initial learning rate as needed\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "model.add(Dropout(0.5))  # Adjust the dropout rate as needed\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "for layer in base_model.layers[-10:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "-OWdB5z2tKjx"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath3 = \"model_checkpoint.h5\"\n",
    "callbacks_checkpoints3 = ModelCheckpoint(\n",
    "\n",
    "    filepath = model_checkpoint_filpath3,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kfFdYLA2tKnJ",
    "outputId": "122daef2-45e0-418a-da3c-e02b744a843b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 32s 421ms/step - loss: 8.2292 - accuracy: 0.3058 - val_loss: 1.3462 - val_accuracy: 0.4563\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 29s 396ms/step - loss: 8.1314 - accuracy: 0.3279 - val_loss: 1.8622 - val_accuracy: 0.4231\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 30s 410ms/step - loss: 7.8330 - accuracy: 0.3531 - val_loss: 0.9587 - val_accuracy: 0.6399\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 29s 398ms/step - loss: 8.1961 - accuracy: 0.3373 - val_loss: 1.2136 - val_accuracy: 0.5516\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 29s 396ms/step - loss: 8.1061 - accuracy: 0.3336 - val_loss: 1.4985 - val_accuracy: 0.5699\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 29s 398ms/step - loss: 7.8584 - accuracy: 0.3581 - val_loss: 1.1158 - val_accuracy: 0.5970\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 29s 401ms/step - loss: 8.2497 - accuracy: 0.3426 - val_loss: 0.9572 - val_accuracy: 0.6512\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 30s 410ms/step - loss: 7.9097 - accuracy: 0.3549 - val_loss: 0.9638 - val_accuracy: 0.6687\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 30s 415ms/step - loss: 8.0630 - accuracy: 0.3391 - val_loss: 0.8636 - val_accuracy: 0.6521\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 30s 413ms/step - loss: 7.9306 - accuracy: 0.3590 - val_loss: 1.0351 - val_accuracy: 0.6477\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 29s 407ms/step - loss: 7.8656 - accuracy: 0.3737 - val_loss: 0.9892 - val_accuracy: 0.6460\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 31s 422ms/step - loss: 7.8372 - accuracy: 0.3654 - val_loss: 0.9123 - val_accuracy: 0.6635\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 31s 428ms/step - loss: 8.1266 - accuracy: 0.3551 - val_loss: 0.8850 - val_accuracy: 0.6836\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 29s 404ms/step - loss: 8.0713 - accuracy: 0.3551 - val_loss: 1.0372 - val_accuracy: 0.5140\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 29s 397ms/step - loss: 7.7421 - accuracy: 0.3719 - val_loss: 1.0150 - val_accuracy: 0.6591\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 29s 398ms/step - loss: 7.8693 - accuracy: 0.3713 - val_loss: 1.0585 - val_accuracy: 0.5997\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 29s 400ms/step - loss: 7.9236 - accuracy: 0.3538 - val_loss: 0.8576 - val_accuracy: 0.6617\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 29s 396ms/step - loss: 7.9182 - accuracy: 0.3669 - val_loss: 1.0859 - val_accuracy: 0.6329\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 30s 410ms/step - loss: 7.8691 - accuracy: 0.3776 - val_loss: 0.8074 - val_accuracy: 0.7019\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 30s 417ms/step - loss: 7.9985 - accuracy: 0.3697 - val_loss: 1.3650 - val_accuracy: 0.5621\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 30s 407ms/step - loss: 8.0135 - accuracy: 0.3687 - val_loss: 0.9245 - val_accuracy: 0.6565\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 29s 395ms/step - loss: 7.8908 - accuracy: 0.3610 - val_loss: 1.5852 - val_accuracy: 0.5017\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 29s 406ms/step - loss: 7.9726 - accuracy: 0.3669 - val_loss: 0.8744 - val_accuracy: 0.6862\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 30s 408ms/step - loss: 8.1088 - accuracy: 0.3573 - val_loss: 0.9088 - val_accuracy: 0.7002\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 30s 413ms/step - loss: 8.0289 - accuracy: 0.3641 - val_loss: 1.1815 - val_accuracy: 0.5542\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 29s 397ms/step - loss: 7.8038 - accuracy: 0.3730 - val_loss: 1.0594 - val_accuracy: 0.5621\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 28s 388ms/step - loss: 7.9748 - accuracy: 0.3660 - val_loss: 1.1057 - val_accuracy: 0.5918\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 29s 394ms/step - loss: 7.9194 - accuracy: 0.3647 - val_loss: 1.0848 - val_accuracy: 0.5647\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 29s 405ms/step - loss: 7.9695 - accuracy: 0.3730 - val_loss: 1.2520 - val_accuracy: 0.5524\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints3, early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7I-4EUd7tKpz",
    "outputId": "fb7f8f15-7f89-4e5c-ca28-b28fe254aec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 5s 249ms/step - loss: 0.8074 - accuracy: 0.7019\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOrQCIVttKtK",
    "outputId": "b97f82b5-65cc-4805-f0c4-a7c5e5533fe3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " global_max_pooling2d_4 (Gl  (None, 512)               0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 512)               262656    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20289092 (77.40 MB)\n",
      "Trainable params: 264708 (1.01 MB)\n",
      "Non-trainable params: 20024384 (76.39 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#VGG19\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained VGG19 model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)\n",
    "\n",
    "# Freeze the layers of the pre-trained VGG19 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your custom model on top of the pre-trained VGG19\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalMaxPooling2D())  # Use GlobalMaxPooling2D instead of MaxPooling2D\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'binary' if it is two classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "CaFL04pitKyX"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath4 = \"model_checkpoint.h5\"\n",
    "callbacks_checkpoints4 = ModelCheckpoint(\n",
    "\n",
    "    filepath = model_checkpoint_filpath4,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "UYF1m_rvtK0_",
    "outputId": "46b7babe-a617-45d5-a6e7-70e88db071cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 43s 586ms/step - loss: 0.9296 - accuracy: 0.6189 - val_loss: 0.6098 - val_accuracy: 0.7579\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 42s 583ms/step - loss: 0.6457 - accuracy: 0.7336 - val_loss: 0.4924 - val_accuracy: 0.8226\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 42s 581ms/step - loss: 0.5575 - accuracy: 0.7791 - val_loss: 0.4956 - val_accuracy: 0.8007\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 42s 585ms/step - loss: 0.5058 - accuracy: 0.7979 - val_loss: 0.5661 - val_accuracy: 0.8191\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 42s 583ms/step - loss: 0.4788 - accuracy: 0.8190 - val_loss: 0.3931 - val_accuracy: 0.8715\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 43s 588ms/step - loss: 0.4493 - accuracy: 0.8260 - val_loss: 0.4974 - val_accuracy: 0.8269\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 42s 583ms/step - loss: 0.4560 - accuracy: 0.8236 - val_loss: 0.4368 - val_accuracy: 0.8409\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 0.4353 - accuracy: 0.8327 - val_loss: 0.4018 - val_accuracy: 0.8566\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 44s 602ms/step - loss: 0.4445 - accuracy: 0.8343 - val_loss: 0.4094 - val_accuracy: 0.8497\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 45s 624ms/step - loss: 0.5076 - accuracy: 0.8109 - val_loss: 0.4891 - val_accuracy: 0.8523\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 46s 628ms/step - loss: 0.4338 - accuracy: 0.8365 - val_loss: 0.5051 - val_accuracy: 0.8418\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 44s 613ms/step - loss: 0.4273 - accuracy: 0.8360 - val_loss: 0.4762 - val_accuracy: 0.8208\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 44s 610ms/step - loss: 0.4122 - accuracy: 0.8459 - val_loss: 0.4744 - val_accuracy: 0.8444\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 45s 621ms/step - loss: 0.4070 - accuracy: 0.8433 - val_loss: 0.4495 - val_accuracy: 0.8365\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 46s 636ms/step - loss: 0.4320 - accuracy: 0.8413 - val_loss: 0.4950 - val_accuracy: 0.8497\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 46s 642ms/step - loss: 0.4138 - accuracy: 0.8398 - val_loss: 0.4574 - val_accuracy: 0.8392\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 48s 662ms/step - loss: 0.4161 - accuracy: 0.8498 - val_loss: 0.4087 - val_accuracy: 0.8505\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 45s 622ms/step - loss: 0.3925 - accuracy: 0.8527 - val_loss: 0.3870 - val_accuracy: 0.8706\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 46s 634ms/step - loss: 0.3752 - accuracy: 0.8549 - val_loss: 0.4851 - val_accuracy: 0.8488\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 47s 645ms/step - loss: 0.3884 - accuracy: 0.8505 - val_loss: 0.4292 - val_accuracy: 0.8706\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 45s 620ms/step - loss: 0.3921 - accuracy: 0.8514 - val_loss: 0.4694 - val_accuracy: 0.8383\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 44s 611ms/step - loss: 0.4271 - accuracy: 0.8378 - val_loss: 0.4261 - val_accuracy: 0.8759\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 46s 631ms/step - loss: 0.4120 - accuracy: 0.8461 - val_loss: 0.4735 - val_accuracy: 0.8497\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 44s 614ms/step - loss: 0.4143 - accuracy: 0.8376 - val_loss: 0.3752 - val_accuracy: 0.8855\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 44s 604ms/step - loss: 0.3896 - accuracy: 0.8603 - val_loss: 0.3743 - val_accuracy: 0.8733\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 45s 615ms/step - loss: 0.3945 - accuracy: 0.8450 - val_loss: 0.3844 - val_accuracy: 0.8846\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 42s 576ms/step - loss: 0.4257 - accuracy: 0.8387 - val_loss: 0.4106 - val_accuracy: 0.8750\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 42s 582ms/step - loss: 0.3851 - accuracy: 0.8533 - val_loss: 0.6243 - val_accuracy: 0.8051\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 42s 575ms/step - loss: 0.3915 - accuracy: 0.8527 - val_loss: 0.4120 - val_accuracy: 0.8785\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 42s 576ms/step - loss: 0.3893 - accuracy: 0.8514 - val_loss: 0.4102 - val_accuracy: 0.8497\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 42s 576ms/step - loss: 0.3732 - accuracy: 0.8603 - val_loss: 0.4248 - val_accuracy: 0.8645\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 42s 576ms/step - loss: 0.3797 - accuracy: 0.8573 - val_loss: 0.4027 - val_accuracy: 0.8733\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 42s 579ms/step - loss: 0.3895 - accuracy: 0.8516 - val_loss: 0.5599 - val_accuracy: 0.8208\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 42s 584ms/step - loss: 0.4013 - accuracy: 0.8463 - val_loss: 0.4952 - val_accuracy: 0.8462\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 43s 589ms/step - loss: 0.3866 - accuracy: 0.8503 - val_loss: 0.4150 - val_accuracy: 0.8540\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 43s 591ms/step - loss: 0.3794 - accuracy: 0.8621 - val_loss: 0.4601 - val_accuracy: 0.8365\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 43s 600ms/step - loss: 0.3760 - accuracy: 0.8595 - val_loss: 0.4321 - val_accuracy: 0.8654\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 44s 603ms/step - loss: 0.3838 - accuracy: 0.8616 - val_loss: 0.3631 - val_accuracy: 0.8829\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 45s 616ms/step - loss: 0.4079 - accuracy: 0.8481 - val_loss: 0.3555 - val_accuracy: 0.8881\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 44s 606ms/step - loss: 0.3947 - accuracy: 0.8496 - val_loss: 0.4278 - val_accuracy: 0.8444\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 44s 609ms/step - loss: 0.3693 - accuracy: 0.8621 - val_loss: 0.4435 - val_accuracy: 0.8654\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 45s 624ms/step - loss: 0.3842 - accuracy: 0.8557 - val_loss: 0.4024 - val_accuracy: 0.8689\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 43s 600ms/step - loss: 0.3728 - accuracy: 0.8568 - val_loss: 0.4822 - val_accuracy: 0.8418\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 44s 606ms/step - loss: 0.4171 - accuracy: 0.8373 - val_loss: 0.4712 - val_accuracy: 0.8584\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 44s 601ms/step - loss: 0.3737 - accuracy: 0.8595 - val_loss: 0.4743 - val_accuracy: 0.8383\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 44s 602ms/step - loss: 0.3677 - accuracy: 0.8588 - val_loss: 0.4023 - val_accuracy: 0.8724\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 44s 601ms/step - loss: 0.3990 - accuracy: 0.8520 - val_loss: 0.3459 - val_accuracy: 0.8907\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 44s 601ms/step - loss: 0.3573 - accuracy: 0.8616 - val_loss: 0.4222 - val_accuracy: 0.8741\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 44s 604ms/step - loss: 0.3832 - accuracy: 0.8601 - val_loss: 0.3884 - val_accuracy: 0.8846\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 44s 604ms/step - loss: 0.3925 - accuracy: 0.8522 - val_loss: 0.4610 - val_accuracy: 0.8287\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 44s 614ms/step - loss: 0.3853 - accuracy: 0.8557 - val_loss: 0.4517 - val_accuracy: 0.8654\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 45s 615ms/step - loss: 0.3865 - accuracy: 0.8448 - val_loss: 0.3989 - val_accuracy: 0.8733\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 45s 618ms/step - loss: 0.3953 - accuracy: 0.8581 - val_loss: 0.4516 - val_accuracy: 0.8488\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 46s 630ms/step - loss: 0.3890 - accuracy: 0.8551 - val_loss: 0.4396 - val_accuracy: 0.8584\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 46s 628ms/step - loss: 0.3846 - accuracy: 0.8520 - val_loss: 0.3494 - val_accuracy: 0.8899\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 45s 621ms/step - loss: 0.3846 - accuracy: 0.8551 - val_loss: 0.3656 - val_accuracy: 0.8829\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 45s 619ms/step - loss: 0.3711 - accuracy: 0.8700 - val_loss: 0.4323 - val_accuracy: 0.8715\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 45s 625ms/step - loss: 0.4043 - accuracy: 0.8496 - val_loss: 0.4296 - val_accuracy: 0.8680\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 46s 630ms/step - loss: 0.3752 - accuracy: 0.8612 - val_loss: 0.3476 - val_accuracy: 0.8872\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 45s 614ms/step - loss: 0.4050 - accuracy: 0.8435 - val_loss: 0.3856 - val_accuracy: 0.8855\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 45s 615ms/step - loss: 0.3794 - accuracy: 0.8575 - val_loss: 0.4073 - val_accuracy: 0.8663\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 45s 616ms/step - loss: 0.3838 - accuracy: 0.8533 - val_loss: 0.4252 - val_accuracy: 0.8610\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 45s 617ms/step - loss: 0.3836 - accuracy: 0.8603 - val_loss: 0.4544 - val_accuracy: 0.8392\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 45s 624ms/step - loss: 0.3768 - accuracy: 0.8577 - val_loss: 0.3953 - val_accuracy: 0.8811\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 44s 621ms/step - loss: 0.3944 - accuracy: 0.8511 - val_loss: 0.4085 - val_accuracy: 0.8645\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 45s 619ms/step - loss: 0.3956 - accuracy: 0.8476 - val_loss: 0.4847 - val_accuracy: 0.8278\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 45s 616ms/step - loss: 0.3984 - accuracy: 0.8577 - val_loss: 0.3811 - val_accuracy: 0.8750\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 45s 620ms/step - loss: 0.3642 - accuracy: 0.8649 - val_loss: 0.4520 - val_accuracy: 0.8636\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 45s 616ms/step - loss: 0.3824 - accuracy: 0.8540 - val_loss: 0.3736 - val_accuracy: 0.8846\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 45s 616ms/step - loss: 0.3697 - accuracy: 0.8627 - val_loss: 0.3588 - val_accuracy: 0.8794\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 45s 622ms/step - loss: 0.3676 - accuracy: 0.8614 - val_loss: 0.3812 - val_accuracy: 0.8671\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 45s 617ms/step - loss: 0.3847 - accuracy: 0.8518 - val_loss: 0.4279 - val_accuracy: 0.8733\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 45s 626ms/step - loss: 0.3785 - accuracy: 0.8588 - val_loss: 0.4417 - val_accuracy: 0.8584\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 45s 621ms/step - loss: 0.3805 - accuracy: 0.8625 - val_loss: 0.5287 - val_accuracy: 0.8365\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 45s 627ms/step - loss: 0.4111 - accuracy: 0.8450 - val_loss: 0.4451 - val_accuracy: 0.8663\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 45s 624ms/step - loss: 0.3861 - accuracy: 0.8566 - val_loss: 0.4525 - val_accuracy: 0.8540\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 46s 630ms/step - loss: 0.3777 - accuracy: 0.8599 - val_loss: 0.4245 - val_accuracy: 0.8715\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 45s 622ms/step - loss: 0.3830 - accuracy: 0.8546 - val_loss: 0.5883 - val_accuracy: 0.8059\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 45s 627ms/step - loss: 0.3868 - accuracy: 0.8518 - val_loss: 0.4382 - val_accuracy: 0.8698\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 45s 627ms/step - loss: 0.3873 - accuracy: 0.8533 - val_loss: 0.3945 - val_accuracy: 0.8689\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 45s 620ms/step - loss: 0.3892 - accuracy: 0.8529 - val_loss: 0.3544 - val_accuracy: 0.8855\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 45s 620ms/step - loss: 0.3744 - accuracy: 0.8610 - val_loss: 0.4290 - val_accuracy: 0.8566\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 44s 615ms/step - loss: 0.3972 - accuracy: 0.8496 - val_loss: 0.3854 - val_accuracy: 0.8741\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 0.3762 - accuracy: 0.8562 - val_loss: 0.3814 - val_accuracy: 0.8540\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 42s 578ms/step - loss: 0.3914 - accuracy: 0.8527 - val_loss: 0.4107 - val_accuracy: 0.8523\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 42s 580ms/step - loss: 0.3868 - accuracy: 0.8511 - val_loss: 0.3590 - val_accuracy: 0.8829\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 42s 581ms/step - loss: 0.3787 - accuracy: 0.8568 - val_loss: 0.3803 - val_accuracy: 0.8837\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 43s 588ms/step - loss: 0.3742 - accuracy: 0.8540 - val_loss: 0.4221 - val_accuracy: 0.8540\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 43s 597ms/step - loss: 0.3888 - accuracy: 0.8514 - val_loss: 0.4177 - val_accuracy: 0.8715\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 43s 589ms/step - loss: 0.3813 - accuracy: 0.8544 - val_loss: 0.3622 - val_accuracy: 0.8829\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 42s 583ms/step - loss: 0.3763 - accuracy: 0.8573 - val_loss: 0.4061 - val_accuracy: 0.8610\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 42s 581ms/step - loss: 0.3682 - accuracy: 0.8579 - val_loss: 0.4460 - val_accuracy: 0.8645\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 42s 580ms/step - loss: 0.3752 - accuracy: 0.8549 - val_loss: 0.4436 - val_accuracy: 0.8706\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 43s 587ms/step - loss: 0.3901 - accuracy: 0.8505 - val_loss: 0.3996 - val_accuracy: 0.8724\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 42s 580ms/step - loss: 0.3991 - accuracy: 0.8492 - val_loss: 0.3737 - val_accuracy: 0.8698\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 45s 615ms/step - loss: 0.3867 - accuracy: 0.8509 - val_loss: 0.3806 - val_accuracy: 0.8689\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 44s 611ms/step - loss: 0.3992 - accuracy: 0.8533 - val_loss: 0.3731 - val_accuracy: 0.8872\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 0.3914 - accuracy: 0.8503 - val_loss: 0.3819 - val_accuracy: 0.8724\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 0.3839 - accuracy: 0.8522 - val_loss: 0.4668 - val_accuracy: 0.8610\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 0.3917 - accuracy: 0.8518 - val_loss: 0.3950 - val_accuracy: 0.8811\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints4]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "av05KQdktK3v",
    "outputId": "2b4c24ac-1c85-4648-d0d8-ebf121c5e35e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 9s 476ms/step - loss: 0.3950 - accuracy: 0.8811\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "MHhF4qAJtK69",
    "outputId": "f8cfcbc8-ce35-4cc9-d724-e89a9ca8e273"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83683744/83683744 [==============================] - 11s 0us/step\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 7, 7, 2048)        20861480  \n",
      "                                                                 \n",
      " global_max_pooling2d_5 (Gl  (None, 2048)              0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21912620 (83.59 MB)\n",
      "Trainable params: 1051140 (4.01 MB)\n",
      "Non-trainable params: 20861480 (79.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Xception\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained Xception model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)\n",
    "\n",
    "# Freeze the layers of the pre-trained Xception model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your custom model on top of the pre-trained Xception\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalMaxPooling2D())  # Use GlobalMaxPooling2D instead of MaxPooling2D\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'binary' if it is two classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "j0e3i5dktK9T"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath5 = \"model_checkpoint.h5\"\n",
    "callbacks_checkpoints5 = ModelCheckpoint(\n",
    "\n",
    "    filepath = model_checkpoint_filpath5,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "BWOAyjgCtK_5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 36s 473ms/step - loss: 3.6965 - accuracy: 0.5944 - val_loss: 1.4682 - val_accuracy: 0.7177\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 32s 443ms/step - loss: 0.8953 - accuracy: 0.7548 - val_loss: 0.7955 - val_accuracy: 0.7893\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 32s 437ms/step - loss: 0.6405 - accuracy: 0.7866 - val_loss: 0.5518 - val_accuracy: 0.8199\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 33s 459ms/step - loss: 0.5191 - accuracy: 0.8220 - val_loss: 0.5317 - val_accuracy: 0.8129\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 32s 441ms/step - loss: 0.5069 - accuracy: 0.8251 - val_loss: 0.5069 - val_accuracy: 0.8252\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.4556 - accuracy: 0.8354 - val_loss: 0.5218 - val_accuracy: 0.8278\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.4332 - accuracy: 0.8389 - val_loss: 0.5586 - val_accuracy: 0.7937\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 33s 453ms/step - loss: 0.4684 - accuracy: 0.8356 - val_loss: 0.4198 - val_accuracy: 0.8601\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 32s 447ms/step - loss: 0.4087 - accuracy: 0.8570 - val_loss: 0.4727 - val_accuracy: 0.8392\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 32s 436ms/step - loss: 0.3967 - accuracy: 0.8619 - val_loss: 0.3882 - val_accuracy: 0.8601\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 33s 450ms/step - loss: 0.3522 - accuracy: 0.8735 - val_loss: 0.4147 - val_accuracy: 0.8514\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 32s 443ms/step - loss: 0.3923 - accuracy: 0.8610 - val_loss: 0.3523 - val_accuracy: 0.8619\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 33s 448ms/step - loss: 0.3882 - accuracy: 0.8575 - val_loss: 0.3860 - val_accuracy: 0.8698\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 33s 451ms/step - loss: 0.3232 - accuracy: 0.8833 - val_loss: 0.3597 - val_accuracy: 0.8706\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 32s 439ms/step - loss: 0.3256 - accuracy: 0.8827 - val_loss: 0.4055 - val_accuracy: 0.8601\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 32s 443ms/step - loss: 0.3784 - accuracy: 0.8671 - val_loss: 0.3668 - val_accuracy: 0.8715\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 33s 458ms/step - loss: 0.3450 - accuracy: 0.8816 - val_loss: 0.3620 - val_accuracy: 0.8636\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.3298 - accuracy: 0.8816 - val_loss: 0.3394 - val_accuracy: 0.8837\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 32s 444ms/step - loss: 0.4219 - accuracy: 0.8601 - val_loss: 0.4768 - val_accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 31s 433ms/step - loss: 0.2803 - accuracy: 0.8873 - val_loss: 0.3279 - val_accuracy: 0.8942\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 33s 456ms/step - loss: 0.3580 - accuracy: 0.8702 - val_loss: 0.3549 - val_accuracy: 0.8698\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 33s 458ms/step - loss: 0.3367 - accuracy: 0.8781 - val_loss: 0.3225 - val_accuracy: 0.8960\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.3049 - accuracy: 0.8881 - val_loss: 0.3871 - val_accuracy: 0.8654\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 33s 459ms/step - loss: 0.3006 - accuracy: 0.8884 - val_loss: 0.3251 - val_accuracy: 0.8794\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.3333 - accuracy: 0.8813 - val_loss: 0.5852 - val_accuracy: 0.8322\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 33s 452ms/step - loss: 0.3869 - accuracy: 0.8776 - val_loss: 0.3477 - val_accuracy: 0.8872\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 33s 453ms/step - loss: 0.3525 - accuracy: 0.8772 - val_loss: 0.4864 - val_accuracy: 0.8339\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 34s 467ms/step - loss: 0.3490 - accuracy: 0.8789 - val_loss: 0.4954 - val_accuracy: 0.8374\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.3429 - accuracy: 0.8827 - val_loss: 0.3061 - val_accuracy: 0.8995\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 32s 439ms/step - loss: 0.2683 - accuracy: 0.9021 - val_loss: 0.2845 - val_accuracy: 0.8995\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 33s 453ms/step - loss: 0.3091 - accuracy: 0.8951 - val_loss: 0.3267 - val_accuracy: 0.9003\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 31s 428ms/step - loss: 0.3317 - accuracy: 0.8859 - val_loss: 0.5831 - val_accuracy: 0.8549\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 32s 436ms/step - loss: 0.2971 - accuracy: 0.9013 - val_loss: 0.3155 - val_accuracy: 0.8907\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 32s 435ms/step - loss: 0.2683 - accuracy: 0.8997 - val_loss: 0.3258 - val_accuracy: 0.8934\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.4568 - accuracy: 0.8573 - val_loss: 0.4243 - val_accuracy: 0.8837\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 32s 444ms/step - loss: 0.3018 - accuracy: 0.8954 - val_loss: 0.3258 - val_accuracy: 0.8951\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 32s 434ms/step - loss: 0.2753 - accuracy: 0.9006 - val_loss: 0.3296 - val_accuracy: 0.8969\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 31s 430ms/step - loss: 0.3153 - accuracy: 0.8967 - val_loss: 0.3017 - val_accuracy: 0.8916\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 34s 473ms/step - loss: 0.2884 - accuracy: 0.8934 - val_loss: 0.3554 - val_accuracy: 0.8741\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 35s 488ms/step - loss: 0.2828 - accuracy: 0.9004 - val_loss: 0.4977 - val_accuracy: 0.8654\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 33s 459ms/step - loss: 0.3527 - accuracy: 0.8853 - val_loss: 0.3065 - val_accuracy: 0.8925\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 35s 482ms/step - loss: 0.2490 - accuracy: 0.9124 - val_loss: 0.3768 - val_accuracy: 0.8733\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 34s 463ms/step - loss: 0.2500 - accuracy: 0.9122 - val_loss: 0.4173 - val_accuracy: 0.8584\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 32s 436ms/step - loss: 0.2622 - accuracy: 0.9076 - val_loss: 0.3905 - val_accuracy: 0.8881\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 33s 457ms/step - loss: 0.2743 - accuracy: 0.8965 - val_loss: 0.3529 - val_accuracy: 0.8890\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 34s 465ms/step - loss: 0.2836 - accuracy: 0.8989 - val_loss: 0.3606 - val_accuracy: 0.8934\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 35s 476ms/step - loss: 0.2801 - accuracy: 0.9017 - val_loss: 0.3447 - val_accuracy: 0.8986\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 34s 474ms/step - loss: 0.3213 - accuracy: 0.8947 - val_loss: 0.3279 - val_accuracy: 0.9065\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 33s 452ms/step - loss: 0.2667 - accuracy: 0.9116 - val_loss: 0.2794 - val_accuracy: 0.9126\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 33s 448ms/step - loss: 0.2741 - accuracy: 0.9072 - val_loss: 0.3305 - val_accuracy: 0.8837\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 33s 460ms/step - loss: 0.3066 - accuracy: 0.8921 - val_loss: 0.3009 - val_accuracy: 0.9065\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 33s 449ms/step - loss: 0.2599 - accuracy: 0.9072 - val_loss: 0.3738 - val_accuracy: 0.8698\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 33s 460ms/step - loss: 0.2957 - accuracy: 0.8967 - val_loss: 0.2992 - val_accuracy: 0.8986\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 32s 439ms/step - loss: 0.3083 - accuracy: 0.8980 - val_loss: 0.3083 - val_accuracy: 0.8899\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 33s 461ms/step - loss: 0.2917 - accuracy: 0.8965 - val_loss: 0.4521 - val_accuracy: 0.8645\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 32s 444ms/step - loss: 0.3502 - accuracy: 0.8890 - val_loss: 0.6609 - val_accuracy: 0.8252\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 33s 448ms/step - loss: 0.3054 - accuracy: 0.9006 - val_loss: 0.3407 - val_accuracy: 0.8872\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 32s 440ms/step - loss: 0.3259 - accuracy: 0.8916 - val_loss: 0.3344 - val_accuracy: 0.9012\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.2365 - accuracy: 0.9170 - val_loss: 0.3199 - val_accuracy: 0.9021\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 34s 464ms/step - loss: 0.3121 - accuracy: 0.8978 - val_loss: 0.3150 - val_accuracy: 0.9108\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 32s 434ms/step - loss: 0.2761 - accuracy: 0.9019 - val_loss: 0.4406 - val_accuracy: 0.8855\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 33s 448ms/step - loss: 0.3052 - accuracy: 0.8967 - val_loss: 0.3408 - val_accuracy: 0.8829\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.2985 - accuracy: 0.8984 - val_loss: 0.4921 - val_accuracy: 0.8759\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 32s 440ms/step - loss: 0.3325 - accuracy: 0.8886 - val_loss: 0.4633 - val_accuracy: 0.8741\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 33s 449ms/step - loss: 0.3527 - accuracy: 0.8877 - val_loss: 0.3607 - val_accuracy: 0.9003\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.2943 - accuracy: 0.9048 - val_loss: 0.5004 - val_accuracy: 0.8549\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 31s 429ms/step - loss: 0.2728 - accuracy: 0.9078 - val_loss: 0.3433 - val_accuracy: 0.8741\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 32s 437ms/step - loss: 0.2433 - accuracy: 0.9157 - val_loss: 0.3315 - val_accuracy: 0.8995\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 31s 428ms/step - loss: 0.2852 - accuracy: 0.9059 - val_loss: 0.4485 - val_accuracy: 0.8750\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 32s 436ms/step - loss: 0.3470 - accuracy: 0.8798 - val_loss: 0.6343 - val_accuracy: 0.8322\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.2528 - accuracy: 0.9148 - val_loss: 0.3194 - val_accuracy: 0.9091\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 32s 438ms/step - loss: 0.2954 - accuracy: 0.9008 - val_loss: 0.3365 - val_accuracy: 0.9030\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 31s 427ms/step - loss: 0.2553 - accuracy: 0.9113 - val_loss: 0.3387 - val_accuracy: 0.8872\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 32s 444ms/step - loss: 0.2640 - accuracy: 0.9092 - val_loss: 0.3042 - val_accuracy: 0.9213\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.2668 - accuracy: 0.9043 - val_loss: 0.5548 - val_accuracy: 0.8470\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 31s 431ms/step - loss: 0.3002 - accuracy: 0.8989 - val_loss: 0.3523 - val_accuracy: 0.9021\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 32s 443ms/step - loss: 0.2735 - accuracy: 0.9100 - val_loss: 0.4818 - val_accuracy: 0.8802\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 33s 450ms/step - loss: 0.3357 - accuracy: 0.8890 - val_loss: 0.3468 - val_accuracy: 0.8951\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.2601 - accuracy: 0.9144 - val_loss: 0.5388 - val_accuracy: 0.8549\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 31s 432ms/step - loss: 0.3247 - accuracy: 0.8930 - val_loss: 0.4644 - val_accuracy: 0.8741\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 31s 431ms/step - loss: 0.3826 - accuracy: 0.8827 - val_loss: 0.4478 - val_accuracy: 0.8942\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 32s 436ms/step - loss: 0.3528 - accuracy: 0.8899 - val_loss: 0.4438 - val_accuracy: 0.8671\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 33s 454ms/step - loss: 0.2844 - accuracy: 0.9076 - val_loss: 0.3502 - val_accuracy: 0.8872\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 31s 425ms/step - loss: 0.2832 - accuracy: 0.9052 - val_loss: 0.4320 - val_accuracy: 0.8907\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 82s 1s/step - loss: 0.2791 - accuracy: 0.9107 - val_loss: 0.5843 - val_accuracy: 0.8628\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 1176s 17s/step - loss: 0.3220 - accuracy: 0.8973 - val_loss: 0.4682 - val_accuracy: 0.8549\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 33s 453ms/step - loss: 0.2908 - accuracy: 0.9063 - val_loss: 0.3969 - val_accuracy: 0.8986\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 33s 455ms/step - loss: 0.2624 - accuracy: 0.9116 - val_loss: 0.3196 - val_accuracy: 0.9030\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 34s 469ms/step - loss: 0.2503 - accuracy: 0.9116 - val_loss: 0.3196 - val_accuracy: 0.8951\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 35s 479ms/step - loss: 0.3423 - accuracy: 0.8949 - val_loss: 0.5007 - val_accuracy: 0.8645\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 34s 473ms/step - loss: 0.2988 - accuracy: 0.8982 - val_loss: 0.3527 - val_accuracy: 0.9082\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 33s 456ms/step - loss: 0.2580 - accuracy: 0.9129 - val_loss: 0.3017 - val_accuracy: 0.9152\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 34s 469ms/step - loss: 0.3149 - accuracy: 0.9032 - val_loss: 0.7679 - val_accuracy: 0.8234\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 35s 488ms/step - loss: 0.3504 - accuracy: 0.8916 - val_loss: 0.6300 - val_accuracy: 0.8531\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 34s 476ms/step - loss: 0.3073 - accuracy: 0.9011 - val_loss: 0.3066 - val_accuracy: 0.9100\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 35s 483ms/step - loss: 0.2821 - accuracy: 0.9078 - val_loss: 0.4230 - val_accuracy: 0.8741\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 34s 464ms/step - loss: 0.3812 - accuracy: 0.8877 - val_loss: 0.3514 - val_accuracy: 0.9021\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 33s 448ms/step - loss: 0.3681 - accuracy: 0.8919 - val_loss: 0.5333 - val_accuracy: 0.8689\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 31s 424ms/step - loss: 0.3009 - accuracy: 0.9065 - val_loss: 0.3382 - val_accuracy: 0.9082\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 33s 452ms/step - loss: 0.2474 - accuracy: 0.9157 - val_loss: 0.3718 - val_accuracy: 0.9030\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints5]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "74s3G-yCtLCU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 6s 335ms/step - loss: 0.3718 - accuracy: 0.9030\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "hhW_eHG6tc5u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 2s 0us/step\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " global_max_pooling2d_6 (Gl  (None, 1280)              0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 512)               655872    \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2915908 (11.12 MB)\n",
      "Trainable params: 657924 (2.51 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#MobileNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained MobileNetV2 model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None, classifier_activation=\"softmax\")\n",
    "\n",
    "# Freeze the layers of the pre-trained MobileNetV2 model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your custom model on top of the pre-trained MobileNetV2\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalMaxPooling2D())  # Use GlobalMaxPooling2D instead of MaxPooling2D\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'binary' if it is two classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "F-u1x6wktc87"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath6 = \"model_checkpoint.h5\"\n",
    "callbacks_checkpoints6 = ModelCheckpoint(\n",
    "\n",
    "    filepath = model_checkpoint_filpath6,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_accuracy\",\n",
    "    mode = \"max\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "lrAtqwAxtdAC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "72/72 [==============================] - 28s 369ms/step - loss: 4.5319 - accuracy: 0.6600 - val_loss: 1.8834 - val_accuracy: 0.7579\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 1.0375 - accuracy: 0.8028 - val_loss: 1.9594 - val_accuracy: 0.7413\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 0.8178 - accuracy: 0.8205 - val_loss: 0.9722 - val_accuracy: 0.8051\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 0.7985 - accuracy: 0.8220 - val_loss: 1.1613 - val_accuracy: 0.7893\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 26s 364ms/step - loss: 0.6370 - accuracy: 0.8413 - val_loss: 0.9757 - val_accuracy: 0.8059\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.5614 - accuracy: 0.8544 - val_loss: 1.5204 - val_accuracy: 0.7535\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 0.5954 - accuracy: 0.8430 - val_loss: 1.2374 - val_accuracy: 0.7675\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.5129 - accuracy: 0.8551 - val_loss: 0.8775 - val_accuracy: 0.7858\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.5319 - accuracy: 0.8546 - val_loss: 0.7349 - val_accuracy: 0.8243\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 0.5408 - accuracy: 0.8608 - val_loss: 0.5983 - val_accuracy: 0.8514\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 0.4604 - accuracy: 0.8708 - val_loss: 0.7927 - val_accuracy: 0.8112\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 0.5002 - accuracy: 0.8623 - val_loss: 1.2391 - val_accuracy: 0.7745\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.5821 - accuracy: 0.8614 - val_loss: 0.4092 - val_accuracy: 0.8759\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 28s 386ms/step - loss: 0.6476 - accuracy: 0.8472 - val_loss: 1.4113 - val_accuracy: 0.7736\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 27s 380ms/step - loss: 0.5320 - accuracy: 0.8643 - val_loss: 0.5056 - val_accuracy: 0.8776\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 28s 385ms/step - loss: 0.4182 - accuracy: 0.8844 - val_loss: 0.7436 - val_accuracy: 0.8269\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 28s 383ms/step - loss: 0.4380 - accuracy: 0.8759 - val_loss: 0.8075 - val_accuracy: 0.8234\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 28s 393ms/step - loss: 0.4611 - accuracy: 0.8741 - val_loss: 1.2082 - val_accuracy: 0.7666\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 27s 372ms/step - loss: 0.5121 - accuracy: 0.8627 - val_loss: 0.7046 - val_accuracy: 0.8514\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 0.4169 - accuracy: 0.8770 - val_loss: 0.6505 - val_accuracy: 0.8409\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 27s 371ms/step - loss: 0.4147 - accuracy: 0.8754 - val_loss: 0.4417 - val_accuracy: 0.8785\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 0.5143 - accuracy: 0.8673 - val_loss: 0.7935 - val_accuracy: 0.8226\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 26s 364ms/step - loss: 0.4917 - accuracy: 0.8750 - val_loss: 0.8317 - val_accuracy: 0.8147\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.4825 - accuracy: 0.8794 - val_loss: 1.0808 - val_accuracy: 0.7911\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 0.7168 - accuracy: 0.8428 - val_loss: 0.4946 - val_accuracy: 0.8802\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 27s 367ms/step - loss: 0.4115 - accuracy: 0.8897 - val_loss: 0.4563 - val_accuracy: 0.8829\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 0.3483 - accuracy: 0.8984 - val_loss: 0.3790 - val_accuracy: 0.8925\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 0.3650 - accuracy: 0.8914 - val_loss: 0.4830 - val_accuracy: 0.8636\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.3477 - accuracy: 0.8923 - val_loss: 0.3700 - val_accuracy: 0.8837\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.3411 - accuracy: 0.8995 - val_loss: 0.4672 - val_accuracy: 0.8837\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.3834 - accuracy: 0.8910 - val_loss: 0.8814 - val_accuracy: 0.8173\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 25s 346ms/step - loss: 0.4574 - accuracy: 0.8750 - val_loss: 0.7459 - val_accuracy: 0.8374\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.4843 - accuracy: 0.8763 - val_loss: 0.3426 - val_accuracy: 0.8995\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.3688 - accuracy: 0.8897 - val_loss: 0.4741 - val_accuracy: 0.8733\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.4345 - accuracy: 0.8818 - val_loss: 0.4905 - val_accuracy: 0.8802\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.3832 - accuracy: 0.8934 - val_loss: 1.2600 - val_accuracy: 0.7719\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 0.4714 - accuracy: 0.8702 - val_loss: 0.5070 - val_accuracy: 0.8645\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.4218 - accuracy: 0.8820 - val_loss: 0.3360 - val_accuracy: 0.8986\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.3415 - accuracy: 0.9013 - val_loss: 1.2347 - val_accuracy: 0.7657\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.3837 - accuracy: 0.8947 - val_loss: 0.6656 - val_accuracy: 0.8531\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.5211 - accuracy: 0.8697 - val_loss: 0.4652 - val_accuracy: 0.8802\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 0.3916 - accuracy: 0.8949 - val_loss: 0.4289 - val_accuracy: 0.8872\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 0.3594 - accuracy: 0.8965 - val_loss: 0.5368 - val_accuracy: 0.8628\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.3806 - accuracy: 0.8901 - val_loss: 1.2122 - val_accuracy: 0.7701\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.3812 - accuracy: 0.8925 - val_loss: 0.4940 - val_accuracy: 0.8829\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.4930 - accuracy: 0.8700 - val_loss: 0.7687 - val_accuracy: 0.8322\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.3637 - accuracy: 0.9006 - val_loss: 0.8631 - val_accuracy: 0.8252\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 26s 353ms/step - loss: 0.3470 - accuracy: 0.8986 - val_loss: 0.4077 - val_accuracy: 0.8986\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.4450 - accuracy: 0.8822 - val_loss: 0.4883 - val_accuracy: 0.8820\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.4461 - accuracy: 0.8792 - val_loss: 0.6786 - val_accuracy: 0.8610\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.4622 - accuracy: 0.8827 - val_loss: 0.5902 - val_accuracy: 0.8733\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.3236 - accuracy: 0.9059 - val_loss: 0.4208 - val_accuracy: 0.8872\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.4186 - accuracy: 0.8827 - val_loss: 1.8083 - val_accuracy: 0.7640\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.5508 - accuracy: 0.8700 - val_loss: 1.0662 - val_accuracy: 0.8024\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.4129 - accuracy: 0.8919 - val_loss: 0.4530 - val_accuracy: 0.8890\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 0.4064 - accuracy: 0.8923 - val_loss: 0.7420 - val_accuracy: 0.8357\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.4126 - accuracy: 0.8901 - val_loss: 0.6215 - val_accuracy: 0.8698\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.2939 - accuracy: 0.9124 - val_loss: 0.4300 - val_accuracy: 0.8916\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 0.3852 - accuracy: 0.8945 - val_loss: 1.7195 - val_accuracy: 0.7343\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.4431 - accuracy: 0.8800 - val_loss: 0.7439 - val_accuracy: 0.8497\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 26s 359ms/step - loss: 0.3609 - accuracy: 0.8997 - val_loss: 0.5721 - val_accuracy: 0.8566\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 0.5503 - accuracy: 0.8682 - val_loss: 0.7214 - val_accuracy: 0.8514\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.4464 - accuracy: 0.8868 - val_loss: 0.7987 - val_accuracy: 0.8217\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 0.4861 - accuracy: 0.8829 - val_loss: 0.4199 - val_accuracy: 0.8942\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.3401 - accuracy: 0.9024 - val_loss: 1.0755 - val_accuracy: 0.8129\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.3453 - accuracy: 0.9028 - val_loss: 0.7912 - val_accuracy: 0.8427\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.3519 - accuracy: 0.9050 - val_loss: 0.3504 - val_accuracy: 0.9082\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.4531 - accuracy: 0.8859 - val_loss: 0.7686 - val_accuracy: 0.8339\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 0.4170 - accuracy: 0.8892 - val_loss: 0.4551 - val_accuracy: 0.8767\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.3897 - accuracy: 0.8934 - val_loss: 0.7520 - val_accuracy: 0.8470\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.5033 - accuracy: 0.8704 - val_loss: 0.8910 - val_accuracy: 0.8182\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 0.4058 - accuracy: 0.8925 - val_loss: 0.5864 - val_accuracy: 0.8663\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 25s 349ms/step - loss: 0.4829 - accuracy: 0.8846 - val_loss: 0.9935 - val_accuracy: 0.8208\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 25s 344ms/step - loss: 0.4918 - accuracy: 0.8835 - val_loss: 0.8093 - val_accuracy: 0.8462\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 0.4284 - accuracy: 0.8949 - val_loss: 0.5076 - val_accuracy: 0.8811\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 25s 348ms/step - loss: 0.4003 - accuracy: 0.8951 - val_loss: 1.4271 - val_accuracy: 0.7815\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.3933 - accuracy: 0.8947 - val_loss: 0.4787 - val_accuracy: 0.8794\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 0.4684 - accuracy: 0.8833 - val_loss: 0.4105 - val_accuracy: 0.8951\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.3632 - accuracy: 0.9032 - val_loss: 0.8268 - val_accuracy: 0.8357\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.3880 - accuracy: 0.8980 - val_loss: 0.9522 - val_accuracy: 0.8182\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 26s 356ms/step - loss: 0.5308 - accuracy: 0.8695 - val_loss: 0.4848 - val_accuracy: 0.8881\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 26s 361ms/step - loss: 0.3917 - accuracy: 0.9000 - val_loss: 1.0672 - val_accuracy: 0.7955\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.3577 - accuracy: 0.9039 - val_loss: 0.6234 - val_accuracy: 0.8741\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 26s 354ms/step - loss: 0.4324 - accuracy: 0.8881 - val_loss: 1.1777 - val_accuracy: 0.8112\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 25s 351ms/step - loss: 0.4706 - accuracy: 0.8864 - val_loss: 0.7120 - val_accuracy: 0.8750\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 25s 350ms/step - loss: 0.5724 - accuracy: 0.8746 - val_loss: 1.4676 - val_accuracy: 0.7911\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.5119 - accuracy: 0.8785 - val_loss: 0.9271 - val_accuracy: 0.8260\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 26s 360ms/step - loss: 0.4948 - accuracy: 0.8838 - val_loss: 1.3505 - val_accuracy: 0.7762\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 27s 380ms/step - loss: 0.6296 - accuracy: 0.8693 - val_loss: 0.7211 - val_accuracy: 0.8698\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 26s 364ms/step - loss: 0.4065 - accuracy: 0.8986 - val_loss: 0.8670 - val_accuracy: 0.8392\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.3428 - accuracy: 0.9059 - val_loss: 0.8200 - val_accuracy: 0.8330\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.3505 - accuracy: 0.9076 - val_loss: 0.6879 - val_accuracy: 0.8671\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 25s 353ms/step - loss: 0.4056 - accuracy: 0.8954 - val_loss: 0.6783 - val_accuracy: 0.8348\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 25s 352ms/step - loss: 0.4285 - accuracy: 0.8940 - val_loss: 0.7582 - val_accuracy: 0.8584\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 26s 357ms/step - loss: 0.4023 - accuracy: 0.8947 - val_loss: 0.9429 - val_accuracy: 0.8313\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 27s 370ms/step - loss: 0.4128 - accuracy: 0.8927 - val_loss: 1.1125 - val_accuracy: 0.8094\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 26s 358ms/step - loss: 0.3774 - accuracy: 0.9015 - val_loss: 0.3524 - val_accuracy: 0.9100\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 27s 371ms/step - loss: 0.4362 - accuracy: 0.8925 - val_loss: 0.9226 - val_accuracy: 0.8287\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 26s 355ms/step - loss: 0.4586 - accuracy: 0.8930 - val_loss: 1.8944 - val_accuracy: 0.7526\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 26s 362ms/step - loss: 0.3931 - accuracy: 0.8986 - val_loss: 0.4236 - val_accuracy: 0.9021\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = epochs,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints6]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "bOhuV58NtdCo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 2s 109ms/step - loss: 0.4236 - accuracy: 0.9021\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(validation_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "R0s_jnMWSWr2"
   },
   "outputs": [],
   "source": [
    "#Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "2xvdfY6ntdFP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyswarms\n",
      "  Downloading pyswarms-1.3.0-py2.py3-none-any.whl.metadata (33 kB)\n",
      "Requirement already satisfied: scipy in ./env/lib/python3.8/site-packages (from pyswarms) (1.10.1)\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (from pyswarms) (1.23.2)\n",
      "Requirement already satisfied: matplotlib>=1.3.1 in ./env/lib/python3.8/site-packages (from pyswarms) (3.7.2)\n",
      "Requirement already satisfied: attrs in ./env/lib/python3.8/site-packages (from pyswarms) (23.1.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.8/site-packages (from pyswarms) (4.66.2)\n",
      "Collecting future (from pyswarms)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.8/site-packages (from pyswarms) (6.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (10.2.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in ./env/lib/python3.8/site-packages (from matplotlib>=1.3.1->pyswarms) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./env/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=1.3.1->pyswarms) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=1.3.1->pyswarms) (1.16.0)\n",
      "Downloading pyswarms-1.3.0-py2.py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: future, pyswarms\n",
      "Successfully installed future-1.0.0 pyswarms-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyswarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "XhXRuhdPScnq"
   },
   "outputs": [],
   "source": [
    "#Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "HD4I7Zt8tdH4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 7, 7, 2048)        20861480  \n",
      "                                                                 \n",
      " global_max_pooling2d_7 (Gl  (None, 2048)              0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21912620 (83.59 MB)\n",
      "Trainable params: 1051140 (4.01 MB)\n",
      "Non-trainable params: 20861480 (79.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, Dense\n",
    "\n",
    "\n",
    "\n",
    "# Load pre-trained Xception model without the top (fully connected) layers\n",
    "base_model = tf.keras.applications.Xception(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3), pooling=None)\n",
    "\n",
    "# Freeze the layers of the pre-trained Xception model\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build your custom model on top of the pre-trained Xception\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalMaxPooling2D())  # Use GlobalMaxPooling2D instead of MaxPooling2D\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))  # Use 'binary' if it is two classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "-6m4d0EktdKp"
   },
   "outputs": [],
   "source": [
    "model_checkpoint_filpath5 = \"model_xception_checkpoint.h5\"\n",
    "callbacks_checkpoints5 = ModelCheckpoint(\n",
    "    filepath = model_checkpoint_filpath5,\n",
    "    save_weights_only = True,\n",
    "    monitor = \"val_loss\",\n",
    "    mode = \"min\",\n",
    "    save_best_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "iKbniHH2tLFD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "72/72 [==============================] - 35s 471ms/step - loss: 3.5593 - accuracy: 0.5976 - val_loss: 1.1266 - val_accuracy: 0.7509\n",
      "Epoch 2/50\n",
      "72/72 [==============================] - 34s 470ms/step - loss: 0.7766 - accuracy: 0.7739 - val_loss: 0.8240 - val_accuracy: 0.7657\n",
      "Epoch 3/50\n",
      "72/72 [==============================] - 34s 466ms/step - loss: 0.6380 - accuracy: 0.7863 - val_loss: 0.5200 - val_accuracy: 0.8252\n",
      "Epoch 4/50\n",
      "72/72 [==============================] - 33s 455ms/step - loss: 0.5002 - accuracy: 0.8308 - val_loss: 0.5556 - val_accuracy: 0.8129\n",
      "Epoch 5/50\n",
      "72/72 [==============================] - 34s 462ms/step - loss: 0.5002 - accuracy: 0.8181 - val_loss: 0.4442 - val_accuracy: 0.8488\n",
      "Epoch 6/50\n",
      "72/72 [==============================] - 34s 474ms/step - loss: 0.4706 - accuracy: 0.8255 - val_loss: 0.5068 - val_accuracy: 0.8217\n",
      "Epoch 7/50\n",
      "72/72 [==============================] - 36s 496ms/step - loss: 0.4799 - accuracy: 0.8282 - val_loss: 0.5140 - val_accuracy: 0.8409\n",
      "Epoch 8/50\n",
      "72/72 [==============================] - 34s 469ms/step - loss: 0.3732 - accuracy: 0.8667 - val_loss: 0.4246 - val_accuracy: 0.8505\n",
      "Epoch 9/50\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.4001 - accuracy: 0.8599 - val_loss: 0.3458 - val_accuracy: 0.8759\n",
      "Epoch 10/50\n",
      "72/72 [==============================] - 33s 456ms/step - loss: 0.4171 - accuracy: 0.8465 - val_loss: 0.5489 - val_accuracy: 0.8147\n",
      "Epoch 11/50\n",
      "72/72 [==============================] - 32s 438ms/step - loss: 0.3277 - accuracy: 0.8743 - val_loss: 0.3475 - val_accuracy: 0.8864\n",
      "Epoch 12/50\n",
      "72/72 [==============================] - 32s 439ms/step - loss: 0.3902 - accuracy: 0.8606 - val_loss: 0.3830 - val_accuracy: 0.8601\n",
      "Epoch 13/50\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.4353 - accuracy: 0.8496 - val_loss: 0.4608 - val_accuracy: 0.8601\n",
      "Epoch 14/50\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.3542 - accuracy: 0.8739 - val_loss: 0.3181 - val_accuracy: 0.8890\n",
      "Epoch 15/50\n",
      "72/72 [==============================] - 36s 502ms/step - loss: 0.3766 - accuracy: 0.8708 - val_loss: 0.5417 - val_accuracy: 0.8278\n",
      "Epoch 16/50\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.3755 - accuracy: 0.8695 - val_loss: 0.3862 - val_accuracy: 0.8785\n",
      "Epoch 17/50\n",
      "72/72 [==============================] - 33s 454ms/step - loss: 0.3714 - accuracy: 0.8700 - val_loss: 0.3888 - val_accuracy: 0.8750\n",
      "Epoch 18/50\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.3328 - accuracy: 0.8822 - val_loss: 0.4259 - val_accuracy: 0.8427\n",
      "Epoch 19/50\n",
      "72/72 [==============================] - 32s 439ms/step - loss: 0.3512 - accuracy: 0.8737 - val_loss: 0.5202 - val_accuracy: 0.8418\n",
      "Epoch 20/50\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.3421 - accuracy: 0.8833 - val_loss: 0.4749 - val_accuracy: 0.8427\n",
      "Epoch 21/50\n",
      "72/72 [==============================] - 32s 435ms/step - loss: 0.3411 - accuracy: 0.8761 - val_loss: 0.3744 - val_accuracy: 0.8837\n",
      "Epoch 22/50\n",
      "72/72 [==============================] - 31s 426ms/step - loss: 0.3735 - accuracy: 0.8669 - val_loss: 0.4326 - val_accuracy: 0.8724\n",
      "Epoch 23/50\n",
      "72/72 [==============================] - 32s 437ms/step - loss: 0.3765 - accuracy: 0.8673 - val_loss: 0.5520 - val_accuracy: 0.8086\n",
      "Epoch 24/50\n",
      "72/72 [==============================] - 31s 431ms/step - loss: 0.3652 - accuracy: 0.8794 - val_loss: 0.5341 - val_accuracy: 0.8243\n",
      "Epoch 25/50\n",
      "72/72 [==============================] - 31s 426ms/step - loss: 0.3093 - accuracy: 0.8901 - val_loss: 0.3077 - val_accuracy: 0.9021\n",
      "Epoch 26/50\n",
      "72/72 [==============================] - 31s 430ms/step - loss: 0.3557 - accuracy: 0.8803 - val_loss: 0.5406 - val_accuracy: 0.8374\n",
      "Epoch 27/50\n",
      "72/72 [==============================] - 32s 440ms/step - loss: 0.3372 - accuracy: 0.8866 - val_loss: 0.3371 - val_accuracy: 0.8907\n",
      "Epoch 28/50\n",
      "72/72 [==============================] - 32s 437ms/step - loss: 0.3394 - accuracy: 0.8855 - val_loss: 0.3632 - val_accuracy: 0.8811\n",
      "Epoch 29/50\n",
      "72/72 [==============================] - 32s 443ms/step - loss: 0.3109 - accuracy: 0.8925 - val_loss: 0.2846 - val_accuracy: 0.9065\n",
      "Epoch 30/50\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.2830 - accuracy: 0.9002 - val_loss: 0.3366 - val_accuracy: 0.9056\n",
      "Epoch 31/50\n",
      "72/72 [==============================] - 31s 430ms/step - loss: 0.3185 - accuracy: 0.8908 - val_loss: 0.2940 - val_accuracy: 0.9073\n",
      "Epoch 32/50\n",
      "72/72 [==============================] - 31s 421ms/step - loss: 0.3353 - accuracy: 0.8840 - val_loss: 0.3228 - val_accuracy: 0.8977\n",
      "Epoch 33/50\n",
      "72/72 [==============================] - 31s 426ms/step - loss: 0.2878 - accuracy: 0.8993 - val_loss: 0.4849 - val_accuracy: 0.8628\n",
      "Epoch 34/50\n",
      "72/72 [==============================] - 31s 431ms/step - loss: 0.3600 - accuracy: 0.8759 - val_loss: 0.3641 - val_accuracy: 0.8733\n",
      "Epoch 35/50\n",
      "72/72 [==============================] - 31s 424ms/step - loss: 0.3120 - accuracy: 0.8936 - val_loss: 0.4795 - val_accuracy: 0.8409\n",
      "Epoch 36/50\n",
      "72/72 [==============================] - 31s 423ms/step - loss: 0.3105 - accuracy: 0.8855 - val_loss: 0.4035 - val_accuracy: 0.8558\n",
      "Epoch 37/50\n",
      "72/72 [==============================] - 31s 427ms/step - loss: 0.3283 - accuracy: 0.8910 - val_loss: 0.3035 - val_accuracy: 0.9012\n",
      "Epoch 38/50\n",
      "72/72 [==============================] - 31s 430ms/step - loss: 0.3293 - accuracy: 0.8857 - val_loss: 0.3123 - val_accuracy: 0.9073\n",
      "Epoch 39/50\n",
      "72/72 [==============================] - 32s 442ms/step - loss: 0.2492 - accuracy: 0.9135 - val_loss: 0.2977 - val_accuracy: 0.8986\n",
      "Epoch 40/50\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.2825 - accuracy: 0.9002 - val_loss: 0.4944 - val_accuracy: 0.8427\n",
      "Epoch 41/50\n",
      "72/72 [==============================] - 33s 449ms/step - loss: 0.2611 - accuracy: 0.9087 - val_loss: 0.2975 - val_accuracy: 0.8986\n",
      "Epoch 42/50\n",
      "72/72 [==============================] - 31s 429ms/step - loss: 0.2598 - accuracy: 0.9085 - val_loss: 0.2973 - val_accuracy: 0.9056\n",
      "Epoch 43/50\n",
      "72/72 [==============================] - 32s 438ms/step - loss: 0.3390 - accuracy: 0.8866 - val_loss: 0.4030 - val_accuracy: 0.8663\n",
      "Epoch 44/50\n",
      "72/72 [==============================] - 32s 446ms/step - loss: 0.3230 - accuracy: 0.8827 - val_loss: 0.3124 - val_accuracy: 0.9030\n",
      "Epoch 45/50\n",
      "72/72 [==============================] - 32s 435ms/step - loss: 0.2620 - accuracy: 0.9109 - val_loss: 0.3952 - val_accuracy: 0.8671\n",
      "Epoch 46/50\n",
      "72/72 [==============================] - 32s 445ms/step - loss: 0.3225 - accuracy: 0.8912 - val_loss: 0.4437 - val_accuracy: 0.8488\n",
      "Epoch 47/50\n",
      "72/72 [==============================] - 34s 469ms/step - loss: 0.2800 - accuracy: 0.9030 - val_loss: 0.3214 - val_accuracy: 0.8925\n",
      "Epoch 48/50\n",
      "72/72 [==============================] - 34s 462ms/step - loss: 0.2642 - accuracy: 0.9072 - val_loss: 0.3673 - val_accuracy: 0.8925\n",
      "Epoch 49/50\n",
      "72/72 [==============================] - 33s 450ms/step - loss: 0.2759 - accuracy: 0.9046 - val_loss: 0.3945 - val_accuracy: 0.8724\n",
      "Epoch 50/50\n",
      "72/72 [==============================] - 32s 435ms/step - loss: 0.3083 - accuracy: 0.8910 - val_loss: 0.3983 - val_accuracy: 0.8907\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    steps_per_epoch = len(train_data),\n",
    "                    epochs = 50,\n",
    "                    validation_data = validation_data,\n",
    "                    validation_steps = len(validation_data),\n",
    "                    callbacks = [callbacks_checkpoints5]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "HyeutBaztpbz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chaithra/tensorflow-test/env/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('final_xception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "hzTT7fGwtpe-"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model_xception_checkpoint.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "y9BTnVyutphw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 6s 340ms/step - loss: 0.2846 - accuracy: 0.9065\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.28463947772979736, 0.9064685106277466]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "JpZ1OcZmtpkv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 26s 344ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4568, 4)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preds = model.predict(train_data)\n",
    "# preds = np.argmax(preds, axis=1)\n",
    "train_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "_PZufsNAtpnd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4568, 4])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_labels = tf.one_hot(train_data.labels, train_data.num_classes)\n",
    "onehot_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "lAwU6LEztpqg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([4568])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = tf.keras.metrics.categorical_crossentropy(onehot_labels, train_preds)\n",
    "train_losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "d_a9QYUatpuC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception_input (InputLayer  [(None, 224, 224, 3)]     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " xception (Functional)       (None, 7, 7, 2048)        20861480  \n",
      "                                                                 \n",
      " global_max_pooling2d_7 (Gl  (None, 2048)              0         \n",
      " obalMaxPooling2D)                                               \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               1049088   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21910568 (83.58 MB)\n",
      "Trainable params: 1049088 (4.00 MB)\n",
      "Non-trainable params: 20861480 (79.58 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = tf.keras.Model(model.input, model.layers[-2].output)\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "IMiqkWaEtpwQ"
   },
   "outputs": [],
   "source": [
    "import pyswarms as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "kOlLBpJctpzf"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"Higher-level method to do forward_prop in the\n",
    "    whole swarm.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    x: numpy.ndarray of shape (n_particles, dimensions)\n",
    "        The swarm that will perform the search\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray of shape (n_particles, )\n",
    "        The computed loss for each particle\n",
    "    \"\"\"\n",
    "    n_particles = x.shape[0]\n",
    "    j = [train_losses[i] for i in range(n_particles)]\n",
    "    return np.array(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "0Tx1T4fBtp2G"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:01:05,006 - pyswarms.single.global_best - INFO - Optimize for 1000 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best: 100%|████████████████|1000/1000, best_cost=0.000614\n",
      "2024-05-06 13:01:37,562 - pyswarms.single.global_best - INFO - Optimization finished | best cost: 0.0006135796429589391, best pos: [0.64679451 0.71792533 0.63119541 ... 0.31223142 0.75440663 0.54957142]\n"
     ]
    }
   ],
   "source": [
    "# Initialize swarm\n",
    "options = {'c1': 0.5, 'c2': 0.3, 'w':0.9}\n",
    "\n",
    "# Call instance of PSO\n",
    "# dimensions = (n_inputs * n_hidden) + (n_hidden * n_classes) + n_hidden + n_classes\n",
    "\n",
    "dimensions = 1024\n",
    "optimizer = ps.single.GlobalBestPSO(n_particles=100, dimensions=dimensions, options=options)\n",
    "\n",
    "# Perform optimization\n",
    "cost, pos = optimizer.optimize(f, iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "_72qLvc5tp47"
   },
   "outputs": [],
   "source": [
    "#Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 219566482655474575\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12768686025825401139\n",
      "physical_device_desc: \"device: 0, name: METAL, pci bus id: <undefined>\"\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "XpsulwsYtqX_"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Stop training on val_acc\n",
    "class EarlyStoppingByAccVal(Callback):\n",
    "    def __init__(self, monitor='val_acc', value=0.00001, verbose=0):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.value = value\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            warnings.warn(\"Early stopping requires %s available!\" % self.monitor, RuntimeWarning)\n",
    "\n",
    "        if current >= self.value:\n",
    "            if self.verbose > 0:\n",
    "                print(\"Epoch %05d: early stopping\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "\n",
    "#Save large model using pickle formate instead of h5\n",
    "class SaveCheckPoint(Callback):\n",
    "    def __init__(self, model, dest_folder):\n",
    "        super(Callback, self).__init__()\n",
    "        self.model = model\n",
    "        self.dest_folder = dest_folder\n",
    "\n",
    "        #initiate\n",
    "        self.best_val_acc = 0\n",
    "        self.best_val_loss = sys.maxsize #get max value\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_acc = logs['val_acc']\n",
    "        val_loss = logs['val_loss']\n",
    "\n",
    "        if val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "\n",
    "            # Save weights in pickle format instead of h5\n",
    "            print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "            weigh= self.model.get_weights()\n",
    "\n",
    "            #now, use pickle to save your model weights, instead of .h5\n",
    "            #for heavy model architectures, .h5 file is unsupported.\n",
    "            fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "            pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "            fpkl.close()\n",
    "\n",
    "#             model.save('tmp.h5')\n",
    "        elif val_acc == self.best_val_acc:\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss=val_loss\n",
    "\n",
    "                # Save weights in pickle format instead of h5\n",
    "                print('\\nSaving val_acc %f at %s' %(self.best_val_acc, self.dest_folder))\n",
    "                weigh= self.model.get_weights()\n",
    "\n",
    "                #now, use pickle to save your model weights, instead of .h5\n",
    "                #for heavy model architectures, .h5 file is unsupported.\n",
    "                fpkl= open(self.dest_folder, 'wb') #Python 3\n",
    "                pickle.dump(weigh, fpkl, protocol= pickle.HIGHEST_PROTOCOL)\n",
    "                fpkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "jF49xHZpymCj"
   },
   "outputs": [],
   "source": [
    "#MUL 1 - Inception - ST\n",
    "\n",
    "from keras.applications import InceptionV3\n",
    "# from keras.applications import Xception\n",
    "# from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "# from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, SimpleRNN, LSTM, Flatten, GRU, Reshape\n",
    "\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "# from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "\n",
    "from keras.layers import GaussianNoise\n",
    "\n",
    "def get_adv_model(num_classes=4):\n",
    "    f1_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(299,299,3))\n",
    "\n",
    "    #frozen layers\n",
    "    for layer in f1_base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    f1_x = f1_base.output\n",
    "    f1_x = GlobalAveragePooling2D()(f1_x)\n",
    "\n",
    "    #Regularization with noise\n",
    "    f1_x = GaussianNoise(0.1)(f1_x)\n",
    "\n",
    "    f1_x = Dense(1024, activation='relu')(f1_x)\n",
    "    f1_x = Dense(num_classes, activation='softmax')(f1_x)\n",
    "    model_1 = Model(inputs=[f1_base.input],outputs=[f1_x])\n",
    "    model_1.summary()\n",
    "\n",
    "    return model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "-5jLyj4EymI6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4568 images belonging to 4 classes.\n",
      "Found 1311 images belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,280 - absl - WARNING - At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n",
      "2024-05-06 13:06:19,283 - absl - WARNING - `lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,371 - tensorflow - INFO - Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs: 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,419 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,422 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,455 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,457 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,476 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,477 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,513 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,515 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,542 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:19,543 - tensorflow - INFO - Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)       [(None, 299, 299, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)          (None, 149, 149, 32)         864       ['input_12[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 149, 149, 32)         96        ['conv2d_23[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)     (None, 149, 149, 32)         0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)          (None, 147, 147, 32)         9216      ['activation[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 147, 147, 32)         96        ['conv2d_24[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)   (None, 147, 147, 32)         0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)          (None, 147, 147, 64)         18432     ['activation_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 147, 147, 64)         192       ['conv2d_25[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 147, 147, 64)         0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooli  (None, 73, 73, 64)           0         ['activation_2[0][0]']        \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)          (None, 73, 73, 80)           5120      ['max_pooling2d_15[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 73, 73, 80)           240       ['conv2d_26[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_3 (Activation)   (None, 73, 73, 80)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)          (None, 71, 71, 192)          138240    ['activation_3[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 71, 71, 192)          576       ['conv2d_27[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_4 (Activation)   (None, 71, 71, 192)          0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_16 (MaxPooli  (None, 35, 35, 192)          0         ['activation_4[0][0]']        \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)          (None, 35, 35, 64)           12288     ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 35, 35, 64)           192       ['conv2d_31[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_8 (Activation)   (None, 35, 35, 64)           0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)          (None, 35, 35, 48)           9216      ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)          (None, 35, 35, 96)           55296     ['activation_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 35, 35, 48)           144       ['conv2d_29[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (Ba  (None, 35, 35, 96)           288       ['conv2d_32[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_6 (Activation)   (None, 35, 35, 48)           0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 35, 35, 96)           0         ['batch_normalization_17[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d (Average  (None, 35, 35, 192)          0         ['max_pooling2d_16[0][0]']    \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)          (None, 35, 35, 64)           12288     ['max_pooling2d_16[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)          (None, 35, 35, 64)           76800     ['activation_6[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)          (None, 35, 35, 96)           82944     ['activation_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)          (None, 35, 35, 32)           6144      ['average_pooling2d[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 35, 35, 64)           192       ['conv2d_28[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 35, 35, 64)           192       ['conv2d_30[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_18 (Ba  (None, 35, 35, 96)           288       ['conv2d_33[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_19 (Ba  (None, 35, 35, 32)           96        ['conv2d_34[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_5 (Activation)   (None, 35, 35, 64)           0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_7 (Activation)   (None, 35, 35, 64)           0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_18[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 35, 35, 32)           0         ['batch_normalization_19[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)        (None, 35, 35, 256)          0         ['activation_5[0][0]',        \n",
      "                                                                     'activation_7[0][0]',        \n",
      "                                                                     'activation_10[0][0]',       \n",
      "                                                                     'activation_11[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)          (None, 35, 35, 64)           16384     ['mixed0[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_23 (Ba  (None, 35, 35, 64)           192       ['conv2d_38[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_15 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_23[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)          (None, 35, 35, 48)           12288     ['mixed0[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)          (None, 35, 35, 96)           55296     ['activation_15[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_21 (Ba  (None, 35, 35, 48)           144       ['conv2d_36[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_24 (Ba  (None, 35, 35, 96)           288       ['conv2d_39[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_13 (Activation)  (None, 35, 35, 48)           0         ['batch_normalization_21[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_16 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_24[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (Avera  (None, 35, 35, 256)          0         ['mixed0[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)          (None, 35, 35, 64)           16384     ['mixed0[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)          (None, 35, 35, 64)           76800     ['activation_13[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)          (None, 35, 35, 96)           82944     ['activation_16[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)          (None, 35, 35, 64)           16384     ['average_pooling2d_1[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_20 (Ba  (None, 35, 35, 64)           192       ['conv2d_35[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_22 (Ba  (None, 35, 35, 64)           192       ['conv2d_37[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_25 (Ba  (None, 35, 35, 96)           288       ['conv2d_40[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_26 (Ba  (None, 35, 35, 64)           192       ['conv2d_41[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_12 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_20[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_14 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_22[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_17 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_25[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_18 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_26[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)        (None, 35, 35, 288)          0         ['activation_12[0][0]',       \n",
      "                                                                     'activation_14[0][0]',       \n",
      "                                                                     'activation_17[0][0]',       \n",
      "                                                                     'activation_18[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)          (None, 35, 35, 64)           18432     ['mixed1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (Ba  (None, 35, 35, 64)           192       ['conv2d_45[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_22 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_30[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)          (None, 35, 35, 48)           13824     ['mixed1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)          (None, 35, 35, 96)           55296     ['activation_22[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_28 (Ba  (None, 35, 35, 48)           144       ['conv2d_43[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_31 (Ba  (None, 35, 35, 96)           288       ['conv2d_46[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_20 (Activation)  (None, 35, 35, 48)           0         ['batch_normalization_28[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_23 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_31[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (Avera  (None, 35, 35, 288)          0         ['mixed1[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)          (None, 35, 35, 64)           18432     ['mixed1[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)          (None, 35, 35, 64)           76800     ['activation_20[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)          (None, 35, 35, 96)           82944     ['activation_23[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)          (None, 35, 35, 64)           18432     ['average_pooling2d_2[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_27 (Ba  (None, 35, 35, 64)           192       ['conv2d_42[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_29 (Ba  (None, 35, 35, 64)           192       ['conv2d_44[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_32 (Ba  (None, 35, 35, 96)           288       ['conv2d_47[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_33 (Ba  (None, 35, 35, 64)           192       ['conv2d_48[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_19 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_27[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_21 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_29[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_24 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_32[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_25 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_33[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)        (None, 35, 35, 288)          0         ['activation_19[0][0]',       \n",
      "                                                                     'activation_21[0][0]',       \n",
      "                                                                     'activation_24[0][0]',       \n",
      "                                                                     'activation_25[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)          (None, 35, 35, 64)           18432     ['mixed2[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_35 (Ba  (None, 35, 35, 64)           192       ['conv2d_50[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_27 (Activation)  (None, 35, 35, 64)           0         ['batch_normalization_35[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)          (None, 35, 35, 96)           55296     ['activation_27[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_36 (Ba  (None, 35, 35, 96)           288       ['conv2d_51[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_28 (Activation)  (None, 35, 35, 96)           0         ['batch_normalization_36[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)          (None, 17, 17, 384)          995328    ['mixed2[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)          (None, 17, 17, 96)           82944     ['activation_28[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_34 (Ba  (None, 17, 17, 384)          1152      ['conv2d_49[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_37 (Ba  (None, 17, 17, 96)           288       ['conv2d_52[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_26 (Activation)  (None, 17, 17, 384)          0         ['batch_normalization_34[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_29 (Activation)  (None, 17, 17, 96)           0         ['batch_normalization_37[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_17 (MaxPooli  (None, 17, 17, 288)          0         ['mixed2[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)        (None, 17, 17, 768)          0         ['activation_26[0][0]',       \n",
      "                                                                     'activation_29[0][0]',       \n",
      "                                                                     'max_pooling2d_17[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)          (None, 17, 17, 128)          98304     ['mixed3[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_42 (Ba  (None, 17, 17, 128)          384       ['conv2d_57[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_34 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_42[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)          (None, 17, 17, 128)          114688    ['activation_34[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_43 (Ba  (None, 17, 17, 128)          384       ['conv2d_58[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_35 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_43[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)          (None, 17, 17, 128)          98304     ['mixed3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)          (None, 17, 17, 128)          114688    ['activation_35[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_39 (Ba  (None, 17, 17, 128)          384       ['conv2d_54[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_44 (Ba  (None, 17, 17, 128)          384       ['conv2d_59[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_31 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_39[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_36 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_44[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)          (None, 17, 17, 128)          114688    ['activation_31[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)          (None, 17, 17, 128)          114688    ['activation_36[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_40 (Ba  (None, 17, 17, 128)          384       ['conv2d_55[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_45 (Ba  (None, 17, 17, 128)          384       ['conv2d_60[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_32 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_40[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_37 (Activation)  (None, 17, 17, 128)          0         ['batch_normalization_45[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (Avera  (None, 17, 17, 768)          0         ['mixed3[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed3[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)          (None, 17, 17, 192)          172032    ['activation_32[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)          (None, 17, 17, 192)          172032    ['activation_37[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)          (None, 17, 17, 192)          147456    ['average_pooling2d_3[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_38 (Ba  (None, 17, 17, 192)          576       ['conv2d_53[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_41 (Ba  (None, 17, 17, 192)          576       ['conv2d_56[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_46 (Ba  (None, 17, 17, 192)          576       ['conv2d_61[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_47 (Ba  (None, 17, 17, 192)          576       ['conv2d_62[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_30 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_38[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_33 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_41[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_38 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_46[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_39 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_47[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)        (None, 17, 17, 768)          0         ['activation_30[0][0]',       \n",
      "                                                                     'activation_33[0][0]',       \n",
      "                                                                     'activation_38[0][0]',       \n",
      "                                                                     'activation_39[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)          (None, 17, 17, 160)          122880    ['mixed4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_52 (Ba  (None, 17, 17, 160)          480       ['conv2d_67[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_44 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_52[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_44[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_53 (Ba  (None, 17, 17, 160)          480       ['conv2d_68[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_45 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_53[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)          (None, 17, 17, 160)          122880    ['mixed4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_45[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_49 (Ba  (None, 17, 17, 160)          480       ['conv2d_64[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_54 (Ba  (None, 17, 17, 160)          480       ['conv2d_69[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_41 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_49[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_46 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_54[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_41[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_46[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_50 (Ba  (None, 17, 17, 160)          480       ['conv2d_65[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_55 (Ba  (None, 17, 17, 160)          480       ['conv2d_70[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_42 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_50[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_47 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_55[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (Avera  (None, 17, 17, 768)          0         ['mixed4[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed4[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)          (None, 17, 17, 192)          215040    ['activation_42[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)          (None, 17, 17, 192)          215040    ['activation_47[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)          (None, 17, 17, 192)          147456    ['average_pooling2d_4[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_48 (Ba  (None, 17, 17, 192)          576       ['conv2d_63[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_51 (Ba  (None, 17, 17, 192)          576       ['conv2d_66[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_56 (Ba  (None, 17, 17, 192)          576       ['conv2d_71[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_57 (Ba  (None, 17, 17, 192)          576       ['conv2d_72[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_40 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_48[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_43 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_51[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_48 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_56[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_49 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_57[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)        (None, 17, 17, 768)          0         ['activation_40[0][0]',       \n",
      "                                                                     'activation_43[0][0]',       \n",
      "                                                                     'activation_48[0][0]',       \n",
      "                                                                     'activation_49[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)          (None, 17, 17, 160)          122880    ['mixed5[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_62 (Ba  (None, 17, 17, 160)          480       ['conv2d_77[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_54 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_62[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_54[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_63 (Ba  (None, 17, 17, 160)          480       ['conv2d_78[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_55 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_63[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)          (None, 17, 17, 160)          122880    ['mixed5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_55[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_59 (Ba  (None, 17, 17, 160)          480       ['conv2d_74[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_64 (Ba  (None, 17, 17, 160)          480       ['conv2d_79[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_51 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_59[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_56 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_64[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_51[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)          (None, 17, 17, 160)          179200    ['activation_56[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_60 (Ba  (None, 17, 17, 160)          480       ['conv2d_75[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_65 (Ba  (None, 17, 17, 160)          480       ['conv2d_80[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_52 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_60[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_57 (Activation)  (None, 17, 17, 160)          0         ['batch_normalization_65[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (Avera  (None, 17, 17, 768)          0         ['mixed5[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed5[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)          (None, 17, 17, 192)          215040    ['activation_52[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)          (None, 17, 17, 192)          215040    ['activation_57[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)          (None, 17, 17, 192)          147456    ['average_pooling2d_5[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_58 (Ba  (None, 17, 17, 192)          576       ['conv2d_73[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_61 (Ba  (None, 17, 17, 192)          576       ['conv2d_76[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_66 (Ba  (None, 17, 17, 192)          576       ['conv2d_81[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_67 (Ba  (None, 17, 17, 192)          576       ['conv2d_82[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_50 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_58[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_53 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_61[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_58 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_66[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_59 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_67[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)        (None, 17, 17, 768)          0         ['activation_50[0][0]',       \n",
      "                                                                     'activation_53[0][0]',       \n",
      "                                                                     'activation_58[0][0]',       \n",
      "                                                                     'activation_59[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed6[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_72 (Ba  (None, 17, 17, 192)          576       ['conv2d_87[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_64 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_72[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_64[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_73 (Ba  (None, 17, 17, 192)          576       ['conv2d_88[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_65 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_73[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_65[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_69 (Ba  (None, 17, 17, 192)          576       ['conv2d_84[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_74 (Ba  (None, 17, 17, 192)          576       ['conv2d_89[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_61 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_69[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_66 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_74[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_61[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_66[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_70 (Ba  (None, 17, 17, 192)          576       ['conv2d_85[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_75 (Ba  (None, 17, 17, 192)          576       ['conv2d_90[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_62 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_70[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_67 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_75[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (Avera  (None, 17, 17, 768)          0         ['mixed6[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed6[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_62[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_67[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)          (None, 17, 17, 192)          147456    ['average_pooling2d_6[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_68 (Ba  (None, 17, 17, 192)          576       ['conv2d_83[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_71 (Ba  (None, 17, 17, 192)          576       ['conv2d_86[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_76 (Ba  (None, 17, 17, 192)          576       ['conv2d_91[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_77 (Ba  (None, 17, 17, 192)          576       ['conv2d_92[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_60 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_68[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_63 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_71[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_68 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_76[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_69 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_77[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)        (None, 17, 17, 768)          0         ['activation_60[0][0]',       \n",
      "                                                                     'activation_63[0][0]',       \n",
      "                                                                     'activation_68[0][0]',       \n",
      "                                                                     'activation_69[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_95 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed7[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_80 (Ba  (None, 17, 17, 192)          576       ['conv2d_95[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_72 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_80[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_96 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_72[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_81 (Ba  (None, 17, 17, 192)          576       ['conv2d_96[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_73 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_81[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)          (None, 17, 17, 192)          147456    ['mixed7[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_97 (Conv2D)          (None, 17, 17, 192)          258048    ['activation_73[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_78 (Ba  (None, 17, 17, 192)          576       ['conv2d_93[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_82 (Ba  (None, 17, 17, 192)          576       ['conv2d_97[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_70 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_78[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_74 (Activation)  (None, 17, 17, 192)          0         ['batch_normalization_82[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 8, 8, 320)            552960    ['activation_70[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_98 (Conv2D)          (None, 8, 8, 192)            331776    ['activation_74[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_79 (Ba  (None, 8, 8, 320)            960       ['conv2d_94[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_83 (Ba  (None, 8, 8, 192)            576       ['conv2d_98[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_71 (Activation)  (None, 8, 8, 320)            0         ['batch_normalization_79[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_75 (Activation)  (None, 8, 8, 192)            0         ['batch_normalization_83[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " max_pooling2d_18 (MaxPooli  (None, 8, 8, 768)            0         ['mixed7[0][0]']              \n",
      " ng2D)                                                                                            \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)        (None, 8, 8, 1280)           0         ['activation_71[0][0]',       \n",
      "                                                                     'activation_75[0][0]',       \n",
      "                                                                     'max_pooling2d_18[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_103 (Conv2D)         (None, 8, 8, 448)            573440    ['mixed8[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_88 (Ba  (None, 8, 8, 448)            1344      ['conv2d_103[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_80 (Activation)  (None, 8, 8, 448)            0         ['batch_normalization_88[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_100 (Conv2D)         (None, 8, 8, 384)            491520    ['mixed8[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_104 (Conv2D)         (None, 8, 8, 384)            1548288   ['activation_80[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_85 (Ba  (None, 8, 8, 384)            1152      ['conv2d_100[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_89 (Ba  (None, 8, 8, 384)            1152      ['conv2d_104[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_77 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_85[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_81 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_89[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_101 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_77[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_102 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_77[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_105 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_81[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_81[0][0]']       \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (Avera  (None, 8, 8, 1280)           0         ['mixed8[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_99 (Conv2D)          (None, 8, 8, 320)            409600    ['mixed8[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_86 (Ba  (None, 8, 8, 384)            1152      ['conv2d_101[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_87 (Ba  (None, 8, 8, 384)            1152      ['conv2d_102[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_90 (Ba  (None, 8, 8, 384)            1152      ['conv2d_105[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_91 (Ba  (None, 8, 8, 384)            1152      ['conv2d_106[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)         (None, 8, 8, 192)            245760    ['average_pooling2d_7[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_84 (Ba  (None, 8, 8, 320)            960       ['conv2d_99[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_78 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_86[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_79 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_87[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_82 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_90[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_83 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_91[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " batch_normalization_92 (Ba  (None, 8, 8, 192)            576       ['conv2d_107[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_76 (Activation)  (None, 8, 8, 320)            0         ['batch_normalization_84[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)      (None, 8, 8, 768)            0         ['activation_78[0][0]',       \n",
      "                                                                     'activation_79[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 8, 8, 768)            0         ['activation_82[0][0]',       \n",
      "                                                                     'activation_83[0][0]']       \n",
      "                                                                                                  \n",
      " activation_84 (Activation)  (None, 8, 8, 192)            0         ['batch_normalization_92[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)        (None, 8, 8, 2048)           0         ['activation_76[0][0]',       \n",
      "                                                                     'mixed9_0[0][0]',            \n",
      "                                                                     'concatenate[0][0]',         \n",
      "                                                                     'activation_84[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)         (None, 8, 8, 448)            917504    ['mixed9[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_97 (Ba  (None, 8, 8, 448)            1344      ['conv2d_112[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_89 (Activation)  (None, 8, 8, 448)            0         ['batch_normalization_97[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)         (None, 8, 8, 384)            786432    ['mixed9[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)         (None, 8, 8, 384)            1548288   ['activation_89[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_94 (Ba  (None, 8, 8, 384)            1152      ['conv2d_109[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_98 (Ba  (None, 8, 8, 384)            1152      ['conv2d_113[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_86 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_94[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_90 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_98[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_86[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_86[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_90[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)         (None, 8, 8, 384)            442368    ['activation_90[0][0]']       \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (Avera  (None, 8, 8, 2048)           0         ['mixed9[0][0]']              \n",
      " gePooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)         (None, 8, 8, 320)            655360    ['mixed9[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_95 (Ba  (None, 8, 8, 384)            1152      ['conv2d_110[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_96 (Ba  (None, 8, 8, 384)            1152      ['conv2d_111[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_99 (Ba  (None, 8, 8, 384)            1152      ['conv2d_114[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " batch_normalization_100 (B  (None, 8, 8, 384)            1152      ['conv2d_115[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " conv2d_116 (Conv2D)         (None, 8, 8, 192)            393216    ['average_pooling2d_8[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_93 (Ba  (None, 8, 8, 320)            960       ['conv2d_108[0][0]']          \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_87 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_95[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_88 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_96[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_91 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_99[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " activation_92 (Activation)  (None, 8, 8, 384)            0         ['batch_normalization_100[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " batch_normalization_101 (B  (None, 8, 8, 192)            576       ['conv2d_116[0][0]']          \n",
      " atchNormalization)                                                                               \n",
      "                                                                                                  \n",
      " activation_85 (Activation)  (None, 8, 8, 320)            0         ['batch_normalization_93[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)      (None, 8, 8, 768)            0         ['activation_87[0][0]',       \n",
      "                                                                     'activation_88[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 8, 8, 768)            0         ['activation_91[0][0]',       \n",
      " )                                                                   'activation_92[0][0]']       \n",
      "                                                                                                  \n",
      " activation_93 (Activation)  (None, 8, 8, 192)            0         ['batch_normalization_101[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)       (None, 8, 8, 2048)           0         ['activation_85[0][0]',       \n",
      "                                                                     'mixed9_1[0][0]',            \n",
      "                                                                     'concatenate_1[0][0]',       \n",
      "                                                                     'activation_93[0][0]']       \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 2048)                 0         ['mixed10[0][0]']             \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNo  (None, 2048)                 0         ['global_average_pooling2d[0][\n",
      " ise)                                                               0]']                          \n",
      "                                                                                                  \n",
      " dense_26 (Dense)            (None, 1024)                 2098176   ['gaussian_noise[0][0]']      \n",
      "                                                                                                  \n",
      " dense_27 (Dense)            (None, 4)                    4100      ['dense_26[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23905060 (91.19 MB)\n",
      "Trainable params: 2102276 (8.02 MB)\n",
      "Non-trainable params: 21802784 (83.17 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 13:06:21,956 - absl - WARNING - There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "import time, os\n",
    "from math import ceil\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # rescale = 1./255,\n",
    "    rotation_range=30,\n",
    "    # width_shift_range=0.3,\n",
    "    # height_shift_range=0.3,\n",
    "    # shear_range=0.3,\n",
    "    # zoom_range=0.3,\n",
    "    # horizontal_flip=True,\n",
    "    # vertical_flip=True,##\n",
    "    # brightness_range=[0.5, 1.5],##\n",
    "    # channel_shift_range=10,##\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    # rescale = 1./255\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "NUM_GPU = 1\n",
    "batch_size = 32\n",
    "\n",
    "train_set = train_datagen.flow_from_directory('/Users/chaithra/tensorflow-test/training_directory',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=7,\n",
    "                                                 # subset=\"training\"\n",
    "                                              )\n",
    "\n",
    "valid_set = test_datagen.flow_from_directory('/Users/chaithra/tensorflow-test/Testing',\n",
    "                                                 target_size = (299, 299),\n",
    "                                                 batch_size = batch_size,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 shuffle=False,\n",
    "                                                 seed=7,\n",
    "                                                 # subset=\"validation\"\n",
    "                                             )\n",
    "\n",
    "model_txt = 'st'\n",
    "# Helper: Save the model.\n",
    "savedfilename = 'Inception_ex01.hdf5'\n",
    "savedfilename_best = 'Inception_ex01_best.hdf5'\n",
    "savedfilename_pre = 'Inception_ex01_pre.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(savedfilename,\n",
    "                          monitor='val_loss', verbose=1,\n",
    "                          save_best_only=False, mode='min',save_weights_only=True)########\n",
    "\n",
    "# Helper: TensorBoard\n",
    "tb = TensorBoard(log_dir=os.path.join('svhn_output', 'logs', model_txt))\n",
    "\n",
    "# Helper: Save results.\n",
    "timestamp = time.time()\n",
    "csv_logger = CSVLogger(os.path.join('svhn_output', 'logs', model_txt + '-' + 'training-' + \\\n",
    "    str(timestamp) + '.log'))\n",
    "\n",
    "earlystopping = EarlyStoppingByAccVal(monitor='val_accuracy', value=0.9900, verbose=1)\n",
    "\n",
    "def rand_scheduler(epoch, lr):\n",
    "    print('1 check epoch')\n",
    "    rnd_lr = 10**(random.uniform(np.log10((1e-3)),np.log10((1e-1))))\n",
    "#     if epoch < 30:\n",
    "#         rnd_lr = 1e-2\n",
    "#     else:\n",
    "#         rnd_lr = 1e-3\n",
    "##     rnd_lr = lr\n",
    "    print('random lr = ', rnd_lr)\n",
    "    return rnd_lr\n",
    "\n",
    "def rand_scheduler2(epoch, lr):\n",
    "    print('2 check epoch')\n",
    "    rnd_lr = 10**(random.uniform(np.log10((1e-5)),np.log10((1e-3))))\n",
    "#     if epoch < 30:\n",
    "#         rnd_lr = 1e-2\n",
    "#     else:\n",
    "#         rnd_lr = 1e-3\n",
    "##     rnd_lr = lr\n",
    "    print('random lr = ', rnd_lr)\n",
    "    return rnd_lr\n",
    "\n",
    "epochs = 40##!!!\n",
    "lr = 1e-2\n",
    "# decay = lr/epochs\n",
    "# optimizer = Adam(lr=lr, decay=decay)\n",
    "# optimizer = Adam(lr=lr)\n",
    "optimizer = SGD(lr=lr)\n",
    "\n",
    "# train on multiple-gpus\n",
    "# Create a MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print(\"Number of GPUs: {}\".format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Open a strategy scope.\n",
    "with strategy.scope():\n",
    "    # Everything that creates variables should be under the strategy scope.\n",
    "    # In general this is only model construction & `compile()`.\n",
    "    model_mul = get_adv_model()\n",
    "    model_mul.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # save initial model\n",
    "    model_mul.save_weights(savedfilename)\n",
    "    model_mul.save_weights(savedfilename_best)\n",
    "    model_mul.save_weights(savedfilename_pre)\n",
    "\n",
    "step_size_train=ceil(train_set.n/train_set.batch_size)\n",
    "step_size_valid=ceil(valid_set.n/valid_set.batch_size)\n",
    "# step_size_test=ceil(testing_set.n//testing_set.batch_size)\n",
    "\n",
    "# result = model_mul.fit_generator(\n",
    "#     generator = train_set,\n",
    "#     steps_per_epoch = step_size_train,\n",
    "#     validation_data = valid_set,\n",
    "#     validation_steps = step_size_valid,\n",
    "#     shuffle=True,\n",
    "#     epochs=epochs,\n",
    "#     callbacks=[checkpointer],\n",
    "# #     callbacks=[csv_logger, checkpointer, earlystopping],\n",
    "# #     callbacks=[tb, csv_logger, checkpointer, earlystopping],\n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "guuzxadwymPg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pbest_value</th>\n",
       "      <th>pbest_file</th>\n",
       "      <th>c_value</th>\n",
       "      <th>c_file</th>\n",
       "      <th>pre_value</th>\n",
       "      <th>pre_file</th>\n",
       "      <th>training_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_best_share1.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_share1.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_pre_share1.hdf5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_best_share2.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_share2.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_pre_share2.hdf5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_best_share3.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_share3.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_pre_share3.hdf5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_best_share4.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_share4.hdf5</td>\n",
       "      <td>0</td>\n",
       "      <td>Inception_ex01_pre_share4.hdf5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pbest_value                       pbest_file  c_value  \\\n",
       "0   1            0  Inception_ex01_best_share1.hdf5        0   \n",
       "1   2            0  Inception_ex01_best_share2.hdf5        0   \n",
       "2   3            0  Inception_ex01_best_share3.hdf5        0   \n",
       "3   4            0  Inception_ex01_best_share4.hdf5        0   \n",
       "\n",
       "                       c_file  pre_value                        pre_file  \\\n",
       "0  Inception_ex01_share1.hdf5          0  Inception_ex01_pre_share1.hdf5   \n",
       "1  Inception_ex01_share2.hdf5          0  Inception_ex01_pre_share2.hdf5   \n",
       "2  Inception_ex01_share3.hdf5          0  Inception_ex01_pre_share3.hdf5   \n",
       "3  Inception_ex01_share4.hdf5          0  Inception_ex01_pre_share4.hdf5   \n",
       "\n",
       "   training_flag  \n",
       "0              0  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# savedfilename = 'Inception_ex01.hdf5'\n",
    "# savedfilename_best = 'Inception_ex01_best.hdf5'\n",
    "# savedfilename_pre = 'Inception_ex01_pre.hdf5'\n",
    "\n",
    "d = {'id': [1, 2, 3, 4], 'pbest_value': [0, 0, 0, 0], 'pbest_file':['Inception_ex01_best_share1.hdf5',\n",
    "                                                        'Inception_ex01_best_share2.hdf5',\n",
    "                                                        'Inception_ex01_best_share3.hdf5',\n",
    "                                                        'Inception_ex01_best_share4.hdf5'],\n",
    "                         'c_value': [0, 0, 0, 0], 'c_file':['Inception_ex01_share1.hdf5',\n",
    "                                                             'Inception_ex01_share2.hdf5',\n",
    "                                                             'Inception_ex01_share3.hdf5',\n",
    "                                                             'Inception_ex01_share4.hdf5'],\n",
    "                         'pre_value': [0, 0, 0, 0], 'pre_file':['Inception_ex01_pre_share1.hdf5',\n",
    "                                                             'Inception_ex01_pre_share2.hdf5',\n",
    "                                                             'Inception_ex01_pre_share3.hdf5',\n",
    "                                                             'Inception_ex01_pre_share4.hdf5'],\n",
    "                         'training_flag':[0, 0, 0, 0]\n",
    "    }\n",
    "df = pandas.DataFrame(data=d)\n",
    "\n",
    "data_file = 'data_ex01.csv'\n",
    "df.to_csv(data_file)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: Replace these lists with your actual model accuracy and loss values\n",
    "epochs = list(range(1, 21))  # Assuming 20 epochs for demonstration\n",
    "\n",
    "# Accuracy for each model\n",
    "accuracy_resnet50 = [0.60, 0.65, 0.66, 0.68, 0.69, 0.70, 0.70, 0.70, 0.70, 0.70, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.71, 0.70, 0.70]\n",
    "accuracy_vgg19 = [0.70, 0.80, 0.84, 0.85, 0.86, 0.87, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88, 0.88]\n",
    "accuracy_xception = [0.75, 0.80, 0.82, 0.83, 0.84, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85, 0.85]\n",
    "accuracy_mobilenet = [0.70, 0.75, 0.76, 0.77, 0.78, 0.79, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80, 0.80]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(epochs, accuracy_resnet50, label='ResNet50', marker='o')\n",
    "plt.plot(epochs, accuracy_vgg19, label='VGG19', marker='o')\n",
    "plt.plot(epochs, accuracy_xception, label='Xception', marker='o')\n",
    "plt.plot(epochs, accuracy_mobilenet, label='MobileNet', marker='o')\n",
    "\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
